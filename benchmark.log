time="2026-01-14T13:53:27Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-14T13:53:28Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-14T13:53:28Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-14T13:53:28Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-14T13:53:28Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-14T13:53:28Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container chroma-server  Creating
 Container chroma-server  Created
 Container chroma-server  Starting
 Container chroma-server  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: chroma
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: chroma on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Chroma: Inserting 1000000 vectors in batches...
   Processed 2000/1000000 vectors...   Processed 12000/1000000 vectors...   Processed 22000/1000000 vectors...   Processed 32000/1000000 vectors...   Processed 42000/1000000 vectors...   Processed 52000/1000000 vectors...   Processed 62000/1000000 vectors...   Processed 72000/1000000 vectors...   Processed 82000/1000000 vectors...   Processed 92000/1000000 vectors...   Processed 102000/1000000 vectors...   Processed 112000/1000000 vectors...   Processed 122000/1000000 vectors...   Processed 132000/1000000 vectors...   Processed 142000/1000000 vectors...   Processed 152000/1000000 vectors...   Processed 162000/1000000 vectors...   Processed 172000/1000000 vectors...   Processed 182000/1000000 vectors...   Processed 192000/1000000 vectors...   Processed 202000/1000000 vectors...   Processed 212000/1000000 vectors...   Processed 222000/1000000 vectors...   Processed 232000/1000000 vectors...   Processed 242000/1000000 vectors...   Processed 252000/1000000 vectors...   Processed 262000/1000000 vectors...   Processed 272000/1000000 vectors...   Processed 282000/1000000 vectors...   Processed 292000/1000000 vectors...   Processed 302000/1000000 vectors...   Processed 312000/1000000 vectors...   Processed 322000/1000000 vectors...   Processed 332000/1000000 vectors...   Processed 342000/1000000 vectors...   Processed 352000/1000000 vectors...   Processed 362000/1000000 vectors...   Processed 372000/1000000 vectors...   Processed 382000/1000000 vectors...   Processed 392000/1000000 vectors...   Processed 402000/1000000 vectors...   Processed 412000/1000000 vectors...   Processed 422000/1000000 vectors...   Processed 432000/1000000 vectors...   Processed 442000/1000000 vectors...   Processed 452000/1000000 vectors...   Processed 462000/1000000 vectors...   Processed 472000/1000000 vectors...   Processed 482000/1000000 vectors...   Processed 492000/1000000 vectors...   Processed 502000/1000000 vectors...   Processed 512000/1000000 vectors...   Processed 522000/1000000 vectors...   Processed 532000/1000000 vectors...   Processed 542000/1000000 vectors...   Processed 552000/1000000 vectors...   Processed 562000/1000000 vectors...   Processed 572000/1000000 vectors...   Processed 582000/1000000 vectors...   Processed 592000/1000000 vectors...   Processed 602000/1000000 vectors...   Processed 612000/1000000 vectors...   Processed 622000/1000000 vectors...   Processed 632000/1000000 vectors...   Processed 642000/1000000 vectors...   Processed 652000/1000000 vectors...   Processed 662000/1000000 vectors...   Processed 672000/1000000 vectors...   Processed 682000/1000000 vectors...   Processed 692000/1000000 vectors...   Processed 702000/1000000 vectors...   Processed 712000/1000000 vectors...   Processed 722000/1000000 vectors...   Processed 732000/1000000 vectors...   Processed 742000/1000000 vectors...   Processed 752000/1000000 vectors...   Processed 762000/1000000 vectors...   Processed 772000/1000000 vectors...   Processed 782000/1000000 vectors...   Processed 792000/1000000 vectors...   Processed 802000/1000000 vectors...   Processed 812000/1000000 vectors...   Processed 822000/1000000 vectors...   Processed 832000/1000000 vectors...   Processed 842000/1000000 vectors...   Processed 852000/1000000 vectors...   Processed 862000/1000000 vectors...   Processed 872000/1000000 vectors...   Processed 882000/1000000 vectors...   Processed 892000/1000000 vectors...   Processed 902000/1000000 vectors...   Processed 912000/1000000 vectors...   Processed 922000/1000000 vectors...   Processed 932000/1000000 vectors...   Processed 942000/1000000 vectors...   Processed 952000/1000000 vectors...   Processed 962000/1000000 vectors...   Processed 972000/1000000 vectors...   Processed 982000/1000000 vectors...   Processed 992000/1000000 vectors...
âœ… Chroma: Insertion complete.
    Run 1: Recall@10=0.1000, Latency_p50=8.76ms
    Run 2: Recall@10=0.1000, Latency_p50=9.21ms
    Run 3: Recall@10=0.1000, Latency_p50=8.25ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
    Results: chroma_gist1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000  â”‚
â”‚ Recall@100       â”‚ 0.6646  â”‚
â”‚ MRR              â”‚ 1.0000  â”‚
â”‚ Latency p50 (ms) â”‚ 8.25    â”‚
â”‚ Latency p99 (ms) â”‚ 11.05   â”‚
â”‚ QPS              â”‚ 119.4   â”‚
â”‚ Build Time (s)   â”‚ 2276.64 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/chroma/gist1m_chroma_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T14:33:37Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container chroma-server  Stopping
 Container chroma-server  Stopped
 Container chroma-server  Removing
 Container chroma-server  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T14:33:45Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container chroma-server  Creating
 Container chroma-server  Created
 Container chroma-server  Starting
 Container chroma-server  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: chroma
Datasets: deep1m
Runs per config: 3


Loading dataset: deep1m

Benchmarking: chroma on deep1m
  Vectors: (1000000, 96), Queries: (10000, 96)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Chroma: Inserting 1000000 vectors in batches...
   Processed 2000/1000000 vectors...   Processed 12000/1000000 vectors...   Processed 22000/1000000 vectors...   Processed 32000/1000000 vectors...   Processed 42000/1000000 vectors...   Processed 52000/1000000 vectors...   Processed 62000/1000000 vectors...   Processed 72000/1000000 vectors...   Processed 82000/1000000 vectors...   Processed 92000/1000000 vectors...   Processed 102000/1000000 vectors...   Processed 112000/1000000 vectors...   Processed 122000/1000000 vectors...   Processed 132000/1000000 vectors...   Processed 142000/1000000 vectors...   Processed 152000/1000000 vectors...   Processed 162000/1000000 vectors...   Processed 172000/1000000 vectors...   Processed 182000/1000000 vectors...   Processed 192000/1000000 vectors...   Processed 202000/1000000 vectors...   Processed 212000/1000000 vectors...   Processed 222000/1000000 vectors...   Processed 232000/1000000 vectors...   Processed 242000/1000000 vectors...   Processed 252000/1000000 vectors...   Processed 262000/1000000 vectors...   Processed 272000/1000000 vectors...   Processed 282000/1000000 vectors...   Processed 292000/1000000 vectors...   Processed 302000/1000000 vectors...   Processed 312000/1000000 vectors...   Processed 322000/1000000 vectors...   Processed 332000/1000000 vectors...   Processed 342000/1000000 vectors...   Processed 352000/1000000 vectors...   Processed 362000/1000000 vectors...   Processed 372000/1000000 vectors...   Processed 382000/1000000 vectors...   Processed 392000/1000000 vectors...   Processed 402000/1000000 vectors...   Processed 412000/1000000 vectors...   Processed 422000/1000000 vectors...   Processed 432000/1000000 vectors...   Processed 442000/1000000 vectors...   Processed 452000/1000000 vectors...   Processed 462000/1000000 vectors...   Processed 472000/1000000 vectors...   Processed 482000/1000000 vectors...   Processed 492000/1000000 vectors...   Processed 502000/1000000 vectors...   Processed 512000/1000000 vectors...   Processed 522000/1000000 vectors...   Processed 532000/1000000 vectors...   Processed 542000/1000000 vectors...   Processed 552000/1000000 vectors...   Processed 562000/1000000 vectors...   Processed 572000/1000000 vectors...   Processed 582000/1000000 vectors...   Processed 592000/1000000 vectors...   Processed 602000/1000000 vectors...   Processed 612000/1000000 vectors...   Processed 622000/1000000 vectors...   Processed 632000/1000000 vectors...   Processed 642000/1000000 vectors...   Processed 652000/1000000 vectors...   Processed 662000/1000000 vectors...   Processed 672000/1000000 vectors...   Processed 682000/1000000 vectors...   Processed 692000/1000000 vectors...   Processed 702000/1000000 vectors...   Processed 712000/1000000 vectors...   Processed 722000/1000000 vectors...   Processed 732000/1000000 vectors...   Processed 742000/1000000 vectors...   Processed 752000/1000000 vectors...   Processed 762000/1000000 vectors...   Processed 772000/1000000 vectors...   Processed 782000/1000000 vectors...   Processed 792000/1000000 vectors...   Processed 802000/1000000 vectors...   Processed 812000/1000000 vectors...   Processed 822000/1000000 vectors...   Processed 832000/1000000 vectors...   Processed 842000/1000000 vectors...   Processed 852000/1000000 vectors...   Processed 862000/1000000 vectors...   Processed 872000/1000000 vectors...   Processed 882000/1000000 vectors...   Processed 892000/1000000 vectors...   Processed 902000/1000000 vectors...   Processed 912000/1000000 vectors...   Processed 922000/1000000 vectors...   Processed 932000/1000000 vectors...   Processed 942000/1000000 vectors...   Processed 952000/1000000 vectors...   Processed 962000/1000000 vectors...   Processed 972000/1000000 vectors...   Processed 982000/1000000 vectors...   Processed 992000/1000000 vectors...
âœ… Chroma: Insertion complete.
    Run 1: Recall@10=0.1000, Latency_p50=12.09ms
    Run 2: Recall@10=0.1000, Latency_p50=10.63ms
    Run 3: Recall@10=0.1000, Latency_p50=8.15ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
    Results: chroma_deep1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000  â”‚
â”‚ Recall@100       â”‚ 0.9053  â”‚
â”‚ MRR              â”‚ 0.9999  â”‚
â”‚ Latency p50 (ms) â”‚ 8.15    â”‚
â”‚ Latency p99 (ms) â”‚ 19.15   â”‚
â”‚ QPS              â”‚ 107.7   â”‚
â”‚ Build Time (s)   â”‚ 1035.44 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/chroma/deep1m_chroma_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T14:58:18Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container chroma-server  Stopping
 Container chroma-server  Stopped
 Container chroma-server  Removing
 Container chroma-server  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T14:58:25Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container chroma-server  Creating
 Container chroma-server  Created
 Container chroma-server  Starting
 Container chroma-server  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: chroma
Datasets: sift1m
Runs per config: 3


Loading dataset: sift1m

Benchmarking: chroma on sift1m
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Chroma: Inserting 1000000 vectors in batches...
   Processed 2000/1000000 vectors...   Processed 12000/1000000 vectors...   Processed 22000/1000000 vectors...   Processed 32000/1000000 vectors...   Processed 42000/1000000 vectors...   Processed 52000/1000000 vectors...   Processed 62000/1000000 vectors...   Processed 72000/1000000 vectors...   Processed 82000/1000000 vectors...   Processed 92000/1000000 vectors...   Processed 102000/1000000 vectors...   Processed 112000/1000000 vectors...   Processed 122000/1000000 vectors...   Processed 132000/1000000 vectors...   Processed 142000/1000000 vectors...   Processed 152000/1000000 vectors...   Processed 162000/1000000 vectors...   Processed 172000/1000000 vectors...   Processed 182000/1000000 vectors...   Processed 192000/1000000 vectors...   Processed 202000/1000000 vectors...   Processed 212000/1000000 vectors...   Processed 222000/1000000 vectors...   Processed 232000/1000000 vectors...   Processed 242000/1000000 vectors...   Processed 252000/1000000 vectors...   Processed 262000/1000000 vectors...   Processed 272000/1000000 vectors...   Processed 282000/1000000 vectors...   Processed 292000/1000000 vectors...   Processed 302000/1000000 vectors...   Processed 312000/1000000 vectors...   Processed 322000/1000000 vectors...   Processed 332000/1000000 vectors...   Processed 342000/1000000 vectors...   Processed 352000/1000000 vectors...   Processed 362000/1000000 vectors...   Processed 372000/1000000 vectors...   Processed 382000/1000000 vectors...   Processed 392000/1000000 vectors...   Processed 402000/1000000 vectors...   Processed 412000/1000000 vectors...   Processed 422000/1000000 vectors...   Processed 432000/1000000 vectors...   Processed 442000/1000000 vectors...   Processed 452000/1000000 vectors...   Processed 462000/1000000 vectors...   Processed 472000/1000000 vectors...   Processed 482000/1000000 vectors...   Processed 492000/1000000 vectors...   Processed 502000/1000000 vectors...   Processed 512000/1000000 vectors...   Processed 522000/1000000 vectors...   Processed 532000/1000000 vectors...   Processed 542000/1000000 vectors...   Processed 552000/1000000 vectors...   Processed 562000/1000000 vectors...   Processed 572000/1000000 vectors...   Processed 582000/1000000 vectors...   Processed 592000/1000000 vectors...   Processed 602000/1000000 vectors...   Processed 612000/1000000 vectors...   Processed 622000/1000000 vectors...   Processed 632000/1000000 vectors...   Processed 642000/1000000 vectors...   Processed 652000/1000000 vectors...   Processed 662000/1000000 vectors...   Processed 672000/1000000 vectors...   Processed 682000/1000000 vectors...   Processed 692000/1000000 vectors...   Processed 702000/1000000 vectors...   Processed 712000/1000000 vectors...   Processed 722000/1000000 vectors...   Processed 732000/1000000 vectors...   Processed 742000/1000000 vectors...   Processed 752000/1000000 vectors...   Processed 762000/1000000 vectors...   Processed 772000/1000000 vectors...   Processed 782000/1000000 vectors...   Processed 792000/1000000 vectors...   Processed 802000/1000000 vectors...   Processed 812000/1000000 vectors...   Processed 822000/1000000 vectors...   Processed 832000/1000000 vectors...   Processed 842000/1000000 vectors...   Processed 852000/1000000 vectors...   Processed 862000/1000000 vectors...   Processed 872000/1000000 vectors...   Processed 882000/1000000 vectors...   Processed 892000/1000000 vectors...   Processed 902000/1000000 vectors...   Processed 912000/1000000 vectors...   Processed 922000/1000000 vectors...   Processed 932000/1000000 vectors...   Processed 942000/1000000 vectors...   Processed 952000/1000000 vectors...   Processed 962000/1000000 vectors...   Processed 972000/1000000 vectors...   Processed 982000/1000000 vectors...   Processed 992000/1000000 vectors...
âœ… Chroma: Insertion complete.
    Run 1: Recall@10=0.1000, Latency_p50=6.94ms
    Run 2: Recall@10=0.1000, Latency_p50=6.95ms
    Run 3: Recall@10=0.1000, Latency_p50=7.16ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
    Results: chroma_sift1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000  â”‚
â”‚ Recall@100       â”‚ 0.9125  â”‚
â”‚ MRR              â”‚ 1.0000  â”‚
â”‚ Latency p50 (ms) â”‚ 7.16    â”‚
â”‚ Latency p99 (ms) â”‚ 11.44   â”‚
â”‚ QPS              â”‚ 134.7   â”‚
â”‚ Build Time (s)   â”‚ 1008.06 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/chroma/sift1m_chroma_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T15:20:28Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container chroma-server  Stopping
 Container chroma-server  Stopped
 Container chroma-server  Removing
 Container chroma-server  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T15:20:34Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container chroma-server  Creating
 Container chroma-server  Created
 Container chroma-server  Starting
 Container chroma-server  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: chroma
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: chroma on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Chroma: Inserting 1000000 vectors in batches...
   Processed 2000/1000000 vectors...   Processed 12000/1000000 vectors...   Processed 22000/1000000 vectors...   Processed 32000/1000000 vectors...   Processed 42000/1000000 vectors...   Processed 52000/1000000 vectors...   Processed 62000/1000000 vectors...   Processed 72000/1000000 vectors...   Processed 82000/1000000 vectors...   Processed 92000/1000000 vectors...   Processed 102000/1000000 vectors...   Processed 112000/1000000 vectors...   Processed 122000/1000000 vectors...   Processed 132000/1000000 vectors...   Processed 142000/1000000 vectors...   Processed 152000/1000000 vectors...   Processed 162000/1000000 vectors...   Processed 172000/1000000 vectors...   Processed 182000/1000000 vectors...   Processed 192000/1000000 vectors...   Processed 202000/1000000 vectors...   Processed 212000/1000000 vectors...   Processed 222000/1000000 vectors...   Processed 232000/1000000 vectors...   Processed 242000/1000000 vectors...   Processed 252000/1000000 vectors...   Processed 262000/1000000 vectors...   Processed 272000/1000000 vectors...   Processed 282000/1000000 vectors...   Processed 292000/1000000 vectors...   Processed 302000/1000000 vectors...   Processed 312000/1000000 vectors...   Processed 322000/1000000 vectors...   Processed 332000/1000000 vectors...   Processed 342000/1000000 vectors...   Processed 352000/1000000 vectors...   Processed 362000/1000000 vectors...   Processed 372000/1000000 vectors...   Processed 382000/1000000 vectors...   Processed 392000/1000000 vectors...   Processed 402000/1000000 vectors...   Processed 412000/1000000 vectors...   Processed 422000/1000000 vectors...   Processed 432000/1000000 vectors...   Processed 442000/1000000 vectors...   Processed 452000/1000000 vectors...   Processed 462000/1000000 vectors...   Processed 472000/1000000 vectors...   Processed 482000/1000000 vectors...   Processed 492000/1000000 vectors...   Processed 502000/1000000 vectors...   Processed 512000/1000000 vectors...   Processed 522000/1000000 vectors...   Processed 532000/1000000 vectors...   Processed 542000/1000000 vectors...   Processed 552000/1000000 vectors...   Processed 562000/1000000 vectors...   Processed 572000/1000000 vectors...   Processed 582000/1000000 vectors...   Processed 592000/1000000 vectors...   Processed 602000/1000000 vectors...   Processed 612000/1000000 vectors...   Processed 622000/1000000 vectors...   Processed 632000/1000000 vectors...   Processed 642000/1000000 vectors...   Processed 652000/1000000 vectors...   Processed 662000/1000000 vectors...   Processed 672000/1000000 vectors...   Processed 682000/1000000 vectors...   Processed 692000/1000000 vectors...   Processed 702000/1000000 vectors...   Processed 712000/1000000 vectors...   Processed 722000/1000000 vectors...   Processed 732000/1000000 vectors...   Processed 742000/1000000 vectors...   Processed 752000/1000000 vectors...   Processed 762000/1000000 vectors...   Processed 772000/1000000 vectors...   Processed 782000/1000000 vectors...   Processed 792000/1000000 vectors...   Processed 802000/1000000 vectors...   Processed 812000/1000000 vectors...   Processed 822000/1000000 vectors...   Processed 832000/1000000 vectors...   Processed 842000/1000000 vectors...   Processed 852000/1000000 vectors...   Processed 862000/1000000 vectors...   Processed 872000/1000000 vectors...   Processed 882000/1000000 vectors...   Processed 892000/1000000 vectors...   Processed 902000/1000000 vectors...   Processed 912000/1000000 vectors...   Processed 922000/1000000 vectors...   Processed 932000/1000000 vectors...   Processed 942000/1000000 vectors...   Processed 952000/1000000 vectors...   Processed 962000/1000000 vectors...   Processed 972000/1000000 vectors...   Processed 982000/1000000 vectors...   Processed 992000/1000000 vectors...
âœ… Chroma: Insertion complete.
    Run 1: Recall@10=0.0998, Latency_p50=7.89ms
    Run 2: Recall@10=0.0998, Latency_p50=7.96ms
    Run 3: Recall@10=0.0998, Latency_p50=8.78ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
    Results: chroma_random    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0998  â”‚
â”‚ Recall@100       â”‚ 0.1972  â”‚
â”‚ MRR              â”‚ 1.0000  â”‚
â”‚ Latency p50 (ms) â”‚ 8.78    â”‚
â”‚ Latency p99 (ms) â”‚ 16.77   â”‚
â”‚ QPS              â”‚ 107.3   â”‚
â”‚ Build Time (s)   â”‚ 1196.41 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/chroma/random_chroma_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T15:46:32Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container chroma-server  Stopping
 Container chroma-server  Stopped
 Container chroma-server  Removing
 Container chroma-server  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T15:46:38Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container chroma-server  Creating
 Container chroma-server  Created
 Container chroma-server  Starting
 Container chroma-server  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: chroma
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: chroma on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Chroma: Inserting 100000 vectors in batches...
   Processed 2000/100000 vectors...   Processed 12000/100000 vectors...   Processed 22000/100000 vectors...   Processed 32000/100000 vectors...   Processed 42000/100000 vectors...   Processed 52000/100000 vectors...   Processed 62000/100000 vectors...   Processed 72000/100000 vectors...   Processed 82000/100000 vectors...   Processed 92000/100000 vectors...
âœ… Chroma: Insertion complete.
    Run 1: Recall@10=0.1000, Latency_p50=7.98ms
    Run 2: Recall@10=0.1000, Latency_p50=7.81ms
    Run 3: Recall@10=0.1000, Latency_p50=7.89ms
   Results: chroma_msmarco   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.8997 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 7.89   â”‚
â”‚ Latency p99 (ms) â”‚ 9.78   â”‚
â”‚ QPS              â”‚ 125.7  â”‚
â”‚ Build Time (s)   â”‚ 95.92  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/chroma/msmarco_chroma_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T15:49:43Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container chroma-server  Stopping
 Container chroma-server  Stopped
 Container chroma-server  Removing
 Container chroma-server  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T15:49:49Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container chroma-server  Creating
 Container chroma-server  Created
 Container chroma-server  Starting
 Container chroma-server  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: chroma
Datasets: glove
Runs per config: 3


Loading dataset: glove

Benchmarking: chroma on glove
  Vectors: (400000, 100), Queries: (10000, 100)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Chroma: Inserting 400000 vectors in batches...
   Processed 2000/400000 vectors...   Processed 12000/400000 vectors...   Processed 22000/400000 vectors...   Processed 32000/400000 vectors...   Processed 42000/400000 vectors...   Processed 52000/400000 vectors...   Processed 62000/400000 vectors...   Processed 72000/400000 vectors...   Processed 82000/400000 vectors...   Processed 92000/400000 vectors...   Processed 102000/400000 vectors...   Processed 112000/400000 vectors...   Processed 122000/400000 vectors...   Processed 132000/400000 vectors...   Processed 142000/400000 vectors...   Processed 152000/400000 vectors...   Processed 162000/400000 vectors...   Processed 172000/400000 vectors...   Processed 182000/400000 vectors...   Processed 192000/400000 vectors...   Processed 202000/400000 vectors...   Processed 212000/400000 vectors...   Processed 222000/400000 vectors...   Processed 232000/400000 vectors...   Processed 242000/400000 vectors...   Processed 252000/400000 vectors...   Processed 262000/400000 vectors...   Processed 272000/400000 vectors...   Processed 282000/400000 vectors...   Processed 292000/400000 vectors...   Processed 302000/400000 vectors...   Processed 312000/400000 vectors...   Processed 322000/400000 vectors...   Processed 332000/400000 vectors...   Processed 342000/400000 vectors...   Processed 352000/400000 vectors...   Processed 362000/400000 vectors...   Processed 372000/400000 vectors...   Processed 382000/400000 vectors...   Processed 392000/400000 vectors...
âœ… Chroma: Insertion complete.
    Run 1: Recall@10=0.0998, Latency_p50=7.24ms
    Run 2: Recall@10=0.0998, Latency_p50=7.29ms
    Run 3: Recall@10=0.0998, Latency_p50=7.10ms
    Results: chroma_glove    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0998 â”‚
â”‚ Recall@100       â”‚ 0.8393 â”‚
â”‚ MRR              â”‚ 0.9986 â”‚
â”‚ Latency p50 (ms) â”‚ 7.10   â”‚
â”‚ Latency p99 (ms) â”‚ 11.97  â”‚
â”‚ QPS              â”‚ 134.5  â”‚
â”‚ Build Time (s)   â”‚ 309.15 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/chroma/glove_chroma_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T16:08:39Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container chroma-server  Stopping
 Container chroma-server  Stopped
 Container chroma-server  Removing
 Container chroma-server  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T16:08:40Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container ragdbeval-weaviate-1  Creating
 Container ragdbeval-weaviate-1  Created
 Container ragdbeval-weaviate-1  Starting
 Container ragdbeval-weaviate-1  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: weaviate
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: weaviate on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Weaviate: Inserting 1000000 vectors (Fixed Batch Mode)...
   Processed 10000 vectors...   Processed 20000 vectors...   Processed 30000 vectors...   Processed 40000 vectors...   Processed 50000 vectors...   Processed 60000 vectors...   Processed 70000 vectors...   Processed 80000 vectors...   Processed 90000 vectors...   Processed 100000 vectors...   Processed 110000 vectors...   Processed 120000 vectors...   Processed 130000 vectors...   Processed 140000 vectors...   Processed 150000 vectors...   Processed 160000 vectors...   Processed 170000 vectors...   Processed 180000 vectors...   Processed 190000 vectors...   Processed 200000 vectors...   Processed 210000 vectors...   Processed 220000 vectors...   Processed 230000 vectors...   Processed 240000 vectors...   Processed 250000 vectors...   Processed 260000 vectors...   Processed 270000 vectors...   Processed 280000 vectors...   Processed 290000 vectors...   Processed 300000 vectors...   Processed 310000 vectors...   Processed 320000 vectors...   Processed 330000 vectors...   Processed 340000 vectors...   Processed 350000 vectors...   Processed 360000 vectors...   Processed 370000 vectors...   Processed 380000 vectors...   Processed 390000 vectors...   Processed 400000 vectors...   Processed 410000 vectors...   Processed 420000 vectors...   Processed 430000 vectors...   Processed 440000 vectors...   Processed 450000 vectors...   Processed 460000 vectors...   Processed 470000 vectors...   Processed 480000 vectors...   Processed 490000 vectors...   Processed 500000 vectors...   Processed 510000 vectors...   Processed 520000 vectors...   Processed 530000 vectors...   Processed 540000 vectors...   Processed 550000 vectors...   Processed 560000 vectors...   Processed 570000 vectors...   Processed 580000 vectors...   Processed 590000 vectors...   Processed 600000 vectors...   Processed 610000 vectors...   Processed 620000 vectors...   Processed 630000 vectors...   Processed 640000 vectors...   Processed 650000 vectors...   Processed 660000 vectors...   Processed 670000 vectors...   Processed 680000 vectors...   Processed 690000 vectors...   Processed 700000 vectors...   Processed 710000 vectors...   Processed 720000 vectors...   Processed 730000 vectors...   Processed 740000 vectors...   Processed 750000 vectors...   Processed 760000 vectors...   Processed 770000 vectors...   Processed 780000 vectors...   Processed 790000 vectors...   Processed 800000 vectors...   Processed 810000 vectors...   Processed 820000 vectors...   Processed 830000 vectors...   Processed 840000 vectors...   Processed 850000 vectors...   Processed 860000 vectors...   Processed 870000 vectors...   Processed 880000 vectors...   Processed 890000 vectors...   Processed 900000 vectors...   Processed 910000 vectors...   Processed 920000 vectors...   Processed 930000 vectors...   Processed 940000 vectors...   Processed 950000 vectors...   Processed 960000 vectors...   Processed 970000 vectors...   Processed 980000 vectors...   Processed 990000 vectors...
â³ Weaviate: Waiting for indexing (Shards READY)...
âœ… All shards READY.
    Run 1: Recall@10=0.1000, Latency_p50=39.87ms
    Run 2: Recall@10=0.1000, Latency_p50=35.08ms
    Run 3: Recall@10=0.1000, Latency_p50=31.22ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: weaviate_gist1m   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000  â”‚
â”‚ Recall@100       â”‚ 0.9091  â”‚
â”‚ MRR              â”‚ 1.0000  â”‚
â”‚ Latency p50 (ms) â”‚ 31.22   â”‚
â”‚ Latency p99 (ms) â”‚ 49.55   â”‚
â”‚ QPS              â”‚ 31.9    â”‚
â”‚ Build Time (s)   â”‚ 1474.01 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/weaviate/gist1m_weaviate_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T16:37:19Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container ragdbeval-weaviate-1  Stopping
 Container ragdbeval-weaviate-1  Stopped
 Container ragdbeval-weaviate-1  Removing
 Container ragdbeval-weaviate-1  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T16:37:27Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container ragdbeval-weaviate-1  Creating
 Container ragdbeval-weaviate-1  Created
 Container ragdbeval-weaviate-1  Starting
 Container ragdbeval-weaviate-1  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: weaviate
Datasets: deep1m
Runs per config: 3


Loading dataset: deep1m

Benchmarking: weaviate on deep1m
  Vectors: (1000000, 96), Queries: (10000, 96)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Weaviate: Inserting 1000000 vectors (Fixed Batch Mode)...
   Processed 10000 vectors...   Processed 20000 vectors...   Processed 30000 vectors...   Processed 40000 vectors...   Processed 50000 vectors...   Processed 60000 vectors...   Processed 70000 vectors...   Processed 80000 vectors...   Processed 90000 vectors...   Processed 100000 vectors...   Processed 110000 vectors...   Processed 120000 vectors...   Processed 130000 vectors...   Processed 140000 vectors...   Processed 150000 vectors...   Processed 160000 vectors...   Processed 170000 vectors...   Processed 180000 vectors...   Processed 190000 vectors...   Processed 200000 vectors...   Processed 210000 vectors...   Processed 220000 vectors...   Processed 230000 vectors...   Processed 240000 vectors...   Processed 250000 vectors...   Processed 260000 vectors...   Processed 270000 vectors...   Processed 280000 vectors...   Processed 290000 vectors...   Processed 300000 vectors...   Processed 310000 vectors...   Processed 320000 vectors...   Processed 330000 vectors...   Processed 340000 vectors...   Processed 350000 vectors...   Processed 360000 vectors...   Processed 370000 vectors...   Processed 380000 vectors...   Processed 390000 vectors...   Processed 400000 vectors...   Processed 410000 vectors...   Processed 420000 vectors...   Processed 430000 vectors...   Processed 440000 vectors...   Processed 450000 vectors...   Processed 460000 vectors...   Processed 470000 vectors...   Processed 480000 vectors...   Processed 490000 vectors...   Processed 500000 vectors...   Processed 510000 vectors...   Processed 520000 vectors...   Processed 530000 vectors...   Processed 540000 vectors...   Processed 550000 vectors...   Processed 560000 vectors...   Processed 570000 vectors...   Processed 580000 vectors...   Processed 590000 vectors...   Processed 600000 vectors...   Processed 610000 vectors...   Processed 620000 vectors...   Processed 630000 vectors...   Processed 640000 vectors...   Processed 650000 vectors...   Processed 660000 vectors...   Processed 670000 vectors...   Processed 680000 vectors...   Processed 690000 vectors...   Processed 700000 vectors...   Processed 710000 vectors...   Processed 720000 vectors...   Processed 730000 vectors...   Processed 740000 vectors...   Processed 750000 vectors...   Processed 760000 vectors...   Processed 770000 vectors...   Processed 780000 vectors...   Processed 790000 vectors...   Processed 800000 vectors...   Processed 810000 vectors...   Processed 820000 vectors...   Processed 830000 vectors...   Processed 840000 vectors...   Processed 850000 vectors...   Processed 860000 vectors...   Processed 870000 vectors...   Processed 880000 vectors...   Processed 890000 vectors...   Processed 900000 vectors...   Processed 910000 vectors...   Processed 920000 vectors...   Processed 930000 vectors...   Processed 940000 vectors...   Processed 950000 vectors...   Processed 960000 vectors...   Processed 970000 vectors...   Processed 980000 vectors...   Processed 990000 vectors...
â³ Weaviate: Waiting for indexing (Shards READY)...
âœ… All shards READY.
    Run 1: Recall@10=0.1000, Latency_p50=12.62ms
    Run 2: Recall@10=0.1000, Latency_p50=12.81ms
    Run 3: Recall@10=0.1000, Latency_p50=12.65ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
  Results: weaviate_deep1m   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9917 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 12.65  â”‚
â”‚ Latency p99 (ms) â”‚ 20.85  â”‚
â”‚ QPS              â”‚ 76.7   â”‚
â”‚ Build Time (s)   â”‚ 822.13 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/weaviate/deep1m_weaviate_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T16:59:42Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container ragdbeval-weaviate-1  Stopping
 Container ragdbeval-weaviate-1  Stopped
 Container ragdbeval-weaviate-1  Removing
 Container ragdbeval-weaviate-1  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T16:59:48Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container ragdbeval-weaviate-1  Creating
 Container ragdbeval-weaviate-1  Created
 Container ragdbeval-weaviate-1  Starting
 Container ragdbeval-weaviate-1  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: weaviate
Datasets: sift1m
Runs per config: 3


Loading dataset: sift1m

Benchmarking: weaviate on sift1m
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Weaviate: Inserting 1000000 vectors (Fixed Batch Mode)...
   Processed 10000 vectors...   Processed 20000 vectors...   Processed 30000 vectors...   Processed 40000 vectors...   Processed 50000 vectors...   Processed 60000 vectors...   Processed 70000 vectors...   Processed 80000 vectors...   Processed 90000 vectors...   Processed 100000 vectors...   Processed 110000 vectors...   Processed 120000 vectors...   Processed 130000 vectors...   Processed 140000 vectors...   Processed 150000 vectors...   Processed 160000 vectors...   Processed 170000 vectors...   Processed 180000 vectors...   Processed 190000 vectors...   Processed 200000 vectors...   Processed 210000 vectors...   Processed 220000 vectors...   Processed 230000 vectors...   Processed 240000 vectors...   Processed 250000 vectors...   Processed 260000 vectors...   Processed 270000 vectors...   Processed 280000 vectors...   Processed 290000 vectors...   Processed 300000 vectors...   Processed 310000 vectors...   Processed 320000 vectors...   Processed 330000 vectors...   Processed 340000 vectors...   Processed 350000 vectors...   Processed 360000 vectors...   Processed 370000 vectors...   Processed 380000 vectors...   Processed 390000 vectors...   Processed 400000 vectors...   Processed 410000 vectors...   Processed 420000 vectors...   Processed 430000 vectors...   Processed 440000 vectors...   Processed 450000 vectors...   Processed 460000 vectors...   Processed 470000 vectors...   Processed 480000 vectors...   Processed 490000 vectors...   Processed 500000 vectors...   Processed 510000 vectors...   Processed 520000 vectors...   Processed 530000 vectors...   Processed 540000 vectors...   Processed 550000 vectors...   Processed 560000 vectors...   Processed 570000 vectors...   Processed 580000 vectors...   Processed 590000 vectors...   Processed 600000 vectors...   Processed 610000 vectors...   Processed 620000 vectors...   Processed 630000 vectors...   Processed 640000 vectors...   Processed 650000 vectors...   Processed 660000 vectors...   Processed 670000 vectors...   Processed 680000 vectors...   Processed 690000 vectors...   Processed 700000 vectors...   Processed 710000 vectors...   Processed 720000 vectors...   Processed 730000 vectors...   Processed 740000 vectors...   Processed 750000 vectors...   Processed 760000 vectors...   Processed 770000 vectors...   Processed 780000 vectors...   Processed 790000 vectors...   Processed 800000 vectors...   Processed 810000 vectors...   Processed 820000 vectors...   Processed 830000 vectors...   Processed 840000 vectors...   Processed 850000 vectors...   Processed 860000 vectors...   Processed 870000 vectors...   Processed 880000 vectors...   Processed 890000 vectors...   Processed 900000 vectors...   Processed 910000 vectors...   Processed 920000 vectors...   Processed 930000 vectors...   Processed 940000 vectors...   Processed 950000 vectors...   Processed 960000 vectors...   Processed 970000 vectors...   Processed 980000 vectors...   Processed 990000 vectors...
â³ Weaviate: Waiting for indexing (Shards READY)...
âœ… All shards READY.
    Run 1: Recall@10=0.1000, Latency_p50=12.48ms
    Run 2: Recall@10=0.1000, Latency_p50=12.28ms
    Run 3: Recall@10=0.1000, Latency_p50=12.54ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
  Results: weaviate_sift1m   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9963 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 12.54  â”‚
â”‚ Latency p99 (ms) â”‚ 20.74  â”‚
â”‚ QPS              â”‚ 77.3   â”‚
â”‚ Build Time (s)   â”‚ 842.32 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/weaviate/sift1m_weaviate_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T17:21:45Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container ragdbeval-weaviate-1  Stopping
 Container ragdbeval-weaviate-1  Stopped
 Container ragdbeval-weaviate-1  Removing
 Container ragdbeval-weaviate-1  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T17:21:52Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container ragdbeval-weaviate-1  Creating
 Container ragdbeval-weaviate-1  Created
 Container ragdbeval-weaviate-1  Starting
 Container ragdbeval-weaviate-1  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: weaviate
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: weaviate on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Weaviate: Inserting 1000000 vectors (Fixed Batch Mode)...
   Processed 10000 vectors...   Processed 20000 vectors...   Processed 30000 vectors...   Processed 40000 vectors...   Processed 50000 vectors...   Processed 60000 vectors...   Processed 70000 vectors...   Processed 80000 vectors...   Processed 90000 vectors...   Processed 100000 vectors...   Processed 110000 vectors...   Processed 120000 vectors...   Processed 130000 vectors...   Processed 140000 vectors...   Processed 150000 vectors...   Processed 160000 vectors...   Processed 170000 vectors...   Processed 180000 vectors...   Processed 190000 vectors...   Processed 200000 vectors...   Processed 210000 vectors...   Processed 220000 vectors...   Processed 230000 vectors...   Processed 240000 vectors...   Processed 250000 vectors...   Processed 260000 vectors...   Processed 270000 vectors...   Processed 280000 vectors...   Processed 290000 vectors...   Processed 300000 vectors...   Processed 310000 vectors...   Processed 320000 vectors...   Processed 330000 vectors...   Processed 340000 vectors...   Processed 350000 vectors...   Processed 360000 vectors...   Processed 370000 vectors...   Processed 380000 vectors...   Processed 390000 vectors...   Processed 400000 vectors...   Processed 410000 vectors...   Processed 420000 vectors...   Processed 430000 vectors...   Processed 440000 vectors...   Processed 450000 vectors...   Processed 460000 vectors...   Processed 470000 vectors...   Processed 480000 vectors...   Processed 490000 vectors...   Processed 500000 vectors...   Processed 510000 vectors...   Processed 520000 vectors...   Processed 530000 vectors...   Processed 540000 vectors...   Processed 550000 vectors...   Processed 560000 vectors...   Processed 570000 vectors...   Processed 580000 vectors...   Processed 590000 vectors...   Processed 600000 vectors...   Processed 610000 vectors...   Processed 620000 vectors...   Processed 630000 vectors...   Processed 640000 vectors...   Processed 650000 vectors...   Processed 660000 vectors...   Processed 670000 vectors...   Processed 680000 vectors...   Processed 690000 vectors...   Processed 700000 vectors...   Processed 710000 vectors...   Processed 720000 vectors...   Processed 730000 vectors...   Processed 740000 vectors...   Processed 750000 vectors...   Processed 760000 vectors...   Processed 770000 vectors...   Processed 780000 vectors...   Processed 790000 vectors...   Processed 800000 vectors...   Processed 810000 vectors...   Processed 820000 vectors...   Processed 830000 vectors...   Processed 840000 vectors...   Processed 850000 vectors...   Processed 860000 vectors...   Processed 870000 vectors...   Processed 880000 vectors...   Processed 890000 vectors...   Processed 900000 vectors...   Processed 910000 vectors...   Processed 920000 vectors...   Processed 930000 vectors...   Processed 940000 vectors...   Processed 950000 vectors...   Processed 960000 vectors...   Processed 970000 vectors...   Processed 980000 vectors...   Processed 990000 vectors...
â³ Weaviate: Waiting for indexing (Shards READY)...
âœ… All shards READY.
    Run 1: Recall@10=0.1000, Latency_p50=17.17ms
    Run 2: Recall@10=0.1000, Latency_p50=17.30ms
    Run 3: Recall@10=0.1000, Latency_p50=17.24ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: weaviate_random   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000  â”‚
â”‚ Recall@100       â”‚ 0.4528  â”‚
â”‚ MRR              â”‚ 1.0000  â”‚
â”‚ Latency p50 (ms) â”‚ 17.24   â”‚
â”‚ Latency p99 (ms) â”‚ 25.28   â”‚
â”‚ QPS              â”‚ 55.9    â”‚
â”‚ Build Time (s)   â”‚ 1412.66 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/weaviate/random_weaviate_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T17:56:00Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container ragdbeval-weaviate-1  Stopping
 Container ragdbeval-weaviate-1  Stopped
 Container ragdbeval-weaviate-1  Removing
 Container ragdbeval-weaviate-1  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T17:56:07Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container ragdbeval-weaviate-1  Creating
 Container ragdbeval-weaviate-1  Created
 Container ragdbeval-weaviate-1  Starting
 Container ragdbeval-weaviate-1  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: weaviate
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: weaviate on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Weaviate: Inserting 100000 vectors (Fixed Batch Mode)...
   Processed 10000 vectors...   Processed 20000 vectors...   Processed 30000 vectors...   Processed 40000 vectors...   Processed 50000 vectors...   Processed 60000 vectors...   Processed 70000 vectors...   Processed 80000 vectors...   Processed 90000 vectors...
â³ Weaviate: Waiting for indexing (Shards READY)...
âœ… All shards READY.
    Run 1: Recall@10=0.1000, Latency_p50=14.55ms
    Run 2: Recall@10=0.1000, Latency_p50=15.81ms
    Run 3: Recall@10=0.1000, Latency_p50=14.74ms
  Results: weaviate_msmarco  
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9862 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 14.74  â”‚
â”‚ Latency p99 (ms) â”‚ 24.81  â”‚
â”‚ QPS              â”‚ 65.7   â”‚
â”‚ Build Time (s)   â”‚ 159.25 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/weaviate/msmarco_weaviate_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T18:00:42Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container ragdbeval-weaviate-1  Stopping
 Container ragdbeval-weaviate-1  Stopped
 Container ragdbeval-weaviate-1  Removing
 Container ragdbeval-weaviate-1  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T18:00:49Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container ragdbeval-weaviate-1  Creating
 Container ragdbeval-weaviate-1  Created
 Container ragdbeval-weaviate-1  Starting
 Container ragdbeval-weaviate-1  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: weaviate
Datasets: glove
Runs per config: 3


Loading dataset: glove

Benchmarking: weaviate on glove
  Vectors: (400000, 100), Queries: (10000, 100)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Weaviate: Inserting 400000 vectors (Fixed Batch Mode)...
   Processed 10000 vectors...   Processed 20000 vectors...   Processed 30000 vectors...   Processed 40000 vectors...   Processed 50000 vectors...   Processed 60000 vectors...   Processed 70000 vectors...   Processed 80000 vectors...   Processed 90000 vectors...   Processed 100000 vectors...   Processed 110000 vectors...   Processed 120000 vectors...   Processed 130000 vectors...   Processed 140000 vectors...   Processed 150000 vectors...   Processed 160000 vectors...   Processed 170000 vectors...   Processed 180000 vectors...   Processed 190000 vectors...   Processed 200000 vectors...   Processed 210000 vectors...   Processed 220000 vectors...   Processed 230000 vectors...   Processed 240000 vectors...   Processed 250000 vectors...   Processed 260000 vectors...   Processed 270000 vectors...   Processed 280000 vectors...   Processed 290000 vectors...   Processed 300000 vectors...   Processed 310000 vectors...   Processed 320000 vectors...   Processed 330000 vectors...   Processed 340000 vectors...   Processed 350000 vectors...   Processed 360000 vectors...   Processed 370000 vectors...   Processed 380000 vectors...   Processed 390000 vectors...
â³ Weaviate: Waiting for indexing (Shards READY)...
âœ… All shards READY.
    Run 1: Recall@10=0.1000, Latency_p50=11.45ms
    Run 2: Recall@10=0.1000, Latency_p50=11.60ms
    Run 3: Recall@10=0.1000, Latency_p50=11.56ms
   Results: weaviate_glove   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9653 â”‚
â”‚ MRR              â”‚ 0.9999 â”‚
â”‚ Latency p50 (ms) â”‚ 11.56  â”‚
â”‚ Latency p99 (ms) â”‚ 18.41  â”‚
â”‚ QPS              â”‚ 84.3   â”‚
â”‚ Build Time (s)   â”‚ 404.62 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/weaviate/glove_weaviate_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T18:22:27Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container ragdbeval-weaviate-1  Stopping
 Container ragdbeval-weaviate-1  Stopped
 Container ragdbeval-weaviate-1  Removing
 Container ragdbeval-weaviate-1  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T18:22:28Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container pgvector_benchmark  Creating
 Container pgvector_benchmark  Created
 Container pgvector_benchmark  Starting
 Container pgvector_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: pgvector
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: pgvector on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Pgvector: Inserting 1000000 vectors in batches...
   Inserted batch 10000/1000000   Inserted batch 20000/1000000   Inserted batch 30000/1000000   Inserted batch 40000/1000000   Inserted batch 50000/1000000   Inserted batch 60000/1000000   Inserted batch 70000/1000000   Inserted batch 80000/1000000   Inserted batch 90000/1000000   Inserted batch 100000/1000000   Inserted batch 110000/1000000   Inserted batch 120000/1000000   Inserted batch 130000/1000000   Inserted batch 140000/1000000   Inserted batch 150000/1000000   Inserted batch 160000/1000000   Inserted batch 170000/1000000   Inserted batch 180000/1000000   Inserted batch 190000/1000000   Inserted batch 200000/1000000   Inserted batch 210000/1000000   Inserted batch 220000/1000000   Inserted batch 230000/1000000   Inserted batch 240000/1000000   Inserted batch 250000/1000000   Inserted batch 260000/1000000   Inserted batch 270000/1000000   Inserted batch 280000/1000000   Inserted batch 290000/1000000   Inserted batch 300000/1000000   Inserted batch 310000/1000000   Inserted batch 320000/1000000   Inserted batch 330000/1000000   Inserted batch 340000/1000000   Inserted batch 350000/1000000   Inserted batch 360000/1000000   Inserted batch 370000/1000000   Inserted batch 380000/1000000   Inserted batch 390000/1000000   Inserted batch 400000/1000000   Inserted batch 410000/1000000   Inserted batch 420000/1000000   Inserted batch 430000/1000000   Inserted batch 440000/1000000   Inserted batch 450000/1000000   Inserted batch 460000/1000000   Inserted batch 470000/1000000   Inserted batch 480000/1000000   Inserted batch 490000/1000000   Inserted batch 500000/1000000   Inserted batch 510000/1000000   Inserted batch 520000/1000000   Inserted batch 530000/1000000   Inserted batch 540000/1000000   Inserted batch 550000/1000000   Inserted batch 560000/1000000   Inserted batch 570000/1000000   Inserted batch 580000/1000000   Inserted batch 590000/1000000   Inserted batch 600000/1000000   Inserted batch 610000/1000000   Inserted batch 620000/1000000   Inserted batch 630000/1000000   Inserted batch 640000/1000000   Inserted batch 650000/1000000   Inserted batch 660000/1000000   Inserted batch 670000/1000000   Inserted batch 680000/1000000   Inserted batch 690000/1000000   Inserted batch 700000/1000000   Inserted batch 710000/1000000   Inserted batch 720000/1000000   Inserted batch 730000/1000000   Inserted batch 740000/1000000   Inserted batch 750000/1000000   Inserted batch 760000/1000000   Inserted batch 770000/1000000   Inserted batch 780000/1000000   Inserted batch 790000/1000000   Inserted batch 800000/1000000   Inserted batch 810000/1000000   Inserted batch 820000/1000000   Inserted batch 830000/1000000   Inserted batch 840000/1000000   Inserted batch 850000/1000000   Inserted batch 860000/1000000   Inserted batch 870000/1000000   Inserted batch 880000/1000000   Inserted batch 890000/1000000   Inserted batch 900000/1000000   Inserted batch 910000/1000000   Inserted batch 920000/1000000   Inserted batch 930000/1000000   Inserted batch 940000/1000000   Inserted batch 950000/1000000   Inserted batch 960000/1000000   Inserted batch 970000/1000000   Inserted batch 980000/1000000   Inserted batch 990000/1000000   Inserted batch 1000000/1000000
âœ… Pgvector: Insertion complete.
âš¡ High dimensionality (960d) detected. Boosting HNSW parameters...
ğŸ”¨ Pgvector: Building hnsw index...
    Run 1: Recall@10=0.1000, Latency_p50=328.15ms
    Run 2: Recall@10=0.1000, Latency_p50=81.34ms
    Run 3: Recall@10=0.1000, Latency_p50=28.11ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: pgvector_gist1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value    â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000   â”‚
â”‚ Recall@100       â”‚ 0.8747   â”‚
â”‚ MRR              â”‚ 1.0000   â”‚
â”‚ Latency p50 (ms) â”‚ 28.11    â”‚
â”‚ Latency p99 (ms) â”‚ 37.65    â”‚
â”‚ QPS              â”‚ 36.3     â”‚
â”‚ Build Time (s)   â”‚ 21704.69 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/pgvector/gist1m_pgvector_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T00:39:26Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container pgvector_benchmark  Stopping
 Container pgvector_benchmark  Stopped
 Container pgvector_benchmark  Removing
 Container pgvector_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T00:39:32Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container pgvector_benchmark  Creating
 Container pgvector_benchmark  Created
 Container pgvector_benchmark  Starting
 Container pgvector_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: pgvector
Datasets: deep1m
Runs per config: 3


Loading dataset: deep1m

Benchmarking: pgvector on deep1m
  Vectors: (1000000, 96), Queries: (10000, 96)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Pgvector: Inserting 1000000 vectors in batches...
   Inserted batch 10000/1000000   Inserted batch 20000/1000000   Inserted batch 30000/1000000   Inserted batch 40000/1000000   Inserted batch 50000/1000000   Inserted batch 60000/1000000   Inserted batch 70000/1000000   Inserted batch 80000/1000000   Inserted batch 90000/1000000   Inserted batch 100000/1000000   Inserted batch 110000/1000000   Inserted batch 120000/1000000   Inserted batch 130000/1000000   Inserted batch 140000/1000000   Inserted batch 150000/1000000   Inserted batch 160000/1000000   Inserted batch 170000/1000000   Inserted batch 180000/1000000   Inserted batch 190000/1000000   Inserted batch 200000/1000000   Inserted batch 210000/1000000   Inserted batch 220000/1000000   Inserted batch 230000/1000000   Inserted batch 240000/1000000   Inserted batch 250000/1000000   Inserted batch 260000/1000000   Inserted batch 270000/1000000   Inserted batch 280000/1000000   Inserted batch 290000/1000000   Inserted batch 300000/1000000   Inserted batch 310000/1000000   Inserted batch 320000/1000000   Inserted batch 330000/1000000   Inserted batch 340000/1000000   Inserted batch 350000/1000000   Inserted batch 360000/1000000   Inserted batch 370000/1000000   Inserted batch 380000/1000000   Inserted batch 390000/1000000   Inserted batch 400000/1000000   Inserted batch 410000/1000000   Inserted batch 420000/1000000   Inserted batch 430000/1000000   Inserted batch 440000/1000000   Inserted batch 450000/1000000   Inserted batch 460000/1000000   Inserted batch 470000/1000000   Inserted batch 480000/1000000   Inserted batch 490000/1000000   Inserted batch 500000/1000000   Inserted batch 510000/1000000   Inserted batch 520000/1000000   Inserted batch 530000/1000000   Inserted batch 540000/1000000   Inserted batch 550000/1000000   Inserted batch 560000/1000000   Inserted batch 570000/1000000   Inserted batch 580000/1000000   Inserted batch 590000/1000000   Inserted batch 600000/1000000   Inserted batch 610000/1000000   Inserted batch 620000/1000000   Inserted batch 630000/1000000   Inserted batch 640000/1000000   Inserted batch 650000/1000000   Inserted batch 660000/1000000   Inserted batch 670000/1000000   Inserted batch 680000/1000000   Inserted batch 690000/1000000   Inserted batch 700000/1000000   Inserted batch 710000/1000000   Inserted batch 720000/1000000   Inserted batch 730000/1000000   Inserted batch 740000/1000000   Inserted batch 750000/1000000   Inserted batch 760000/1000000   Inserted batch 770000/1000000   Inserted batch 780000/1000000   Inserted batch 790000/1000000   Inserted batch 800000/1000000   Inserted batch 810000/1000000   Inserted batch 820000/1000000   Inserted batch 830000/1000000   Inserted batch 840000/1000000   Inserted batch 850000/1000000   Inserted batch 860000/1000000   Inserted batch 870000/1000000   Inserted batch 880000/1000000   Inserted batch 890000/1000000   Inserted batch 900000/1000000   Inserted batch 910000/1000000   Inserted batch 920000/1000000   Inserted batch 930000/1000000   Inserted batch 940000/1000000   Inserted batch 950000/1000000   Inserted batch 960000/1000000   Inserted batch 970000/1000000   Inserted batch 980000/1000000   Inserted batch 990000/1000000   Inserted batch 1000000/1000000
âœ… Pgvector: Insertion complete.
ğŸ”¨ Pgvector: Building hnsw index...
    Run 1: Recall@10=0.1000, Latency_p50=4.65ms
    Run 2: Recall@10=0.1000, Latency_p50=4.70ms
    Run 3: Recall@10=0.1000, Latency_p50=4.70ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
  Results: pgvector_deep1m   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9293 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 4.70   â”‚
â”‚ Latency p99 (ms) â”‚ 6.47   â”‚
â”‚ QPS              â”‚ 215.0  â”‚
â”‚ Build Time (s)   â”‚ 727.48 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/pgvector/deep1m_pgvector_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T00:55:19Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container pgvector_benchmark  Stopping
 Container pgvector_benchmark  Stopped
 Container pgvector_benchmark  Removing
 Container pgvector_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T00:55:25Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container pgvector_benchmark  Creating
 Container pgvector_benchmark  Created
 Container pgvector_benchmark  Starting
 Container pgvector_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: pgvector
Datasets: sift1m
Runs per config: 3


Loading dataset: sift1m

Benchmarking: pgvector on sift1m
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Pgvector: Inserting 1000000 vectors in batches...
   Inserted batch 10000/1000000   Inserted batch 20000/1000000   Inserted batch 30000/1000000   Inserted batch 40000/1000000   Inserted batch 50000/1000000   Inserted batch 60000/1000000   Inserted batch 70000/1000000   Inserted batch 80000/1000000   Inserted batch 90000/1000000   Inserted batch 100000/1000000   Inserted batch 110000/1000000   Inserted batch 120000/1000000   Inserted batch 130000/1000000   Inserted batch 140000/1000000   Inserted batch 150000/1000000   Inserted batch 160000/1000000   Inserted batch 170000/1000000   Inserted batch 180000/1000000   Inserted batch 190000/1000000   Inserted batch 200000/1000000   Inserted batch 210000/1000000   Inserted batch 220000/1000000   Inserted batch 230000/1000000   Inserted batch 240000/1000000   Inserted batch 250000/1000000   Inserted batch 260000/1000000   Inserted batch 270000/1000000   Inserted batch 280000/1000000   Inserted batch 290000/1000000   Inserted batch 300000/1000000   Inserted batch 310000/1000000   Inserted batch 320000/1000000   Inserted batch 330000/1000000   Inserted batch 340000/1000000   Inserted batch 350000/1000000   Inserted batch 360000/1000000   Inserted batch 370000/1000000   Inserted batch 380000/1000000   Inserted batch 390000/1000000   Inserted batch 400000/1000000   Inserted batch 410000/1000000   Inserted batch 420000/1000000   Inserted batch 430000/1000000   Inserted batch 440000/1000000   Inserted batch 450000/1000000   Inserted batch 460000/1000000   Inserted batch 470000/1000000   Inserted batch 480000/1000000   Inserted batch 490000/1000000   Inserted batch 500000/1000000   Inserted batch 510000/1000000   Inserted batch 520000/1000000   Inserted batch 530000/1000000   Inserted batch 540000/1000000   Inserted batch 550000/1000000   Inserted batch 560000/1000000   Inserted batch 570000/1000000   Inserted batch 580000/1000000   Inserted batch 590000/1000000   Inserted batch 600000/1000000   Inserted batch 610000/1000000   Inserted batch 620000/1000000   Inserted batch 630000/1000000   Inserted batch 640000/1000000   Inserted batch 650000/1000000   Inserted batch 660000/1000000   Inserted batch 670000/1000000   Inserted batch 680000/1000000   Inserted batch 690000/1000000   Inserted batch 700000/1000000   Inserted batch 710000/1000000   Inserted batch 720000/1000000   Inserted batch 730000/1000000   Inserted batch 740000/1000000   Inserted batch 750000/1000000   Inserted batch 760000/1000000   Inserted batch 770000/1000000   Inserted batch 780000/1000000   Inserted batch 790000/1000000   Inserted batch 800000/1000000   Inserted batch 810000/1000000   Inserted batch 820000/1000000   Inserted batch 830000/1000000   Inserted batch 840000/1000000   Inserted batch 850000/1000000   Inserted batch 860000/1000000   Inserted batch 870000/1000000   Inserted batch 880000/1000000   Inserted batch 890000/1000000   Inserted batch 900000/1000000   Inserted batch 910000/1000000   Inserted batch 920000/1000000   Inserted batch 930000/1000000   Inserted batch 940000/1000000   Inserted batch 950000/1000000   Inserted batch 960000/1000000   Inserted batch 970000/1000000   Inserted batch 980000/1000000   Inserted batch 990000/1000000   Inserted batch 1000000/1000000
âœ… Pgvector: Insertion complete.
ğŸ”¨ Pgvector: Building hnsw index...
    Run 1: Recall@10=0.1000, Latency_p50=4.45ms
    Run 2: Recall@10=0.1000, Latency_p50=4.47ms
    Run 3: Recall@10=0.1000, Latency_p50=4.50ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
  Results: pgvector_sift1m   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9367 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 4.50   â”‚
â”‚ Latency p99 (ms) â”‚ 6.16   â”‚
â”‚ QPS              â”‚ 225.1  â”‚
â”‚ Build Time (s)   â”‚ 675.26 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/pgvector/sift1m_pgvector_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T01:10:10Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container pgvector_benchmark  Stopping
 Container pgvector_benchmark  Stopped
 Container pgvector_benchmark  Removing
 Container pgvector_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T01:10:17Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container pgvector_benchmark  Creating
 Container pgvector_benchmark  Created
 Container pgvector_benchmark  Starting
 Container pgvector_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: pgvector
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: pgvector on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Pgvector: Inserting 1000000 vectors in batches...
   Inserted batch 10000/1000000   Inserted batch 20000/1000000   Inserted batch 30000/1000000   Inserted batch 40000/1000000   Inserted batch 50000/1000000   Inserted batch 60000/1000000   Inserted batch 70000/1000000   Inserted batch 80000/1000000   Inserted batch 90000/1000000   Inserted batch 100000/1000000   Inserted batch 110000/1000000   Inserted batch 120000/1000000   Inserted batch 130000/1000000   Inserted batch 140000/1000000   Inserted batch 150000/1000000   Inserted batch 160000/1000000   Inserted batch 170000/1000000   Inserted batch 180000/1000000   Inserted batch 190000/1000000   Inserted batch 200000/1000000   Inserted batch 210000/1000000   Inserted batch 220000/1000000   Inserted batch 230000/1000000   Inserted batch 240000/1000000   Inserted batch 250000/1000000   Inserted batch 260000/1000000   Inserted batch 270000/1000000   Inserted batch 280000/1000000   Inserted batch 290000/1000000   Inserted batch 300000/1000000   Inserted batch 310000/1000000   Inserted batch 320000/1000000   Inserted batch 330000/1000000   Inserted batch 340000/1000000   Inserted batch 350000/1000000   Inserted batch 360000/1000000   Inserted batch 370000/1000000   Inserted batch 380000/1000000   Inserted batch 390000/1000000   Inserted batch 400000/1000000   Inserted batch 410000/1000000   Inserted batch 420000/1000000   Inserted batch 430000/1000000   Inserted batch 440000/1000000   Inserted batch 450000/1000000   Inserted batch 460000/1000000   Inserted batch 470000/1000000   Inserted batch 480000/1000000   Inserted batch 490000/1000000   Inserted batch 500000/1000000   Inserted batch 510000/1000000   Inserted batch 520000/1000000   Inserted batch 530000/1000000   Inserted batch 540000/1000000   Inserted batch 550000/1000000   Inserted batch 560000/1000000   Inserted batch 570000/1000000   Inserted batch 580000/1000000   Inserted batch 590000/1000000   Inserted batch 600000/1000000   Inserted batch 610000/1000000   Inserted batch 620000/1000000   Inserted batch 630000/1000000   Inserted batch 640000/1000000   Inserted batch 650000/1000000   Inserted batch 660000/1000000   Inserted batch 670000/1000000   Inserted batch 680000/1000000   Inserted batch 690000/1000000   Inserted batch 700000/1000000   Inserted batch 710000/1000000   Inserted batch 720000/1000000   Inserted batch 730000/1000000   Inserted batch 740000/1000000   Inserted batch 750000/1000000   Inserted batch 760000/1000000   Inserted batch 770000/1000000   Inserted batch 780000/1000000   Inserted batch 790000/1000000   Inserted batch 800000/1000000   Inserted batch 810000/1000000   Inserted batch 820000/1000000   Inserted batch 830000/1000000   Inserted batch 840000/1000000   Inserted batch 850000/1000000   Inserted batch 860000/1000000   Inserted batch 870000/1000000   Inserted batch 880000/1000000   Inserted batch 890000/1000000   Inserted batch 900000/1000000   Inserted batch 910000/1000000   Inserted batch 920000/1000000   Inserted batch 930000/1000000   Inserted batch 940000/1000000   Inserted batch 950000/1000000   Inserted batch 960000/1000000   Inserted batch 970000/1000000   Inserted batch 980000/1000000   Inserted batch 990000/1000000   Inserted batch 1000000/1000000
âœ… Pgvector: Insertion complete.
ğŸ”¨ Pgvector: Building hnsw index...
    Run 1: Recall@10=0.0999, Latency_p50=7.08ms
    Run 2: Recall@10=0.0999, Latency_p50=6.61ms
    Run 3: Recall@10=0.0999, Latency_p50=6.51ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
  Results: pgvector_random   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0999 â”‚
â”‚ Recall@100       â”‚ 0.2039 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 6.51   â”‚
â”‚ Latency p99 (ms) â”‚ 8.72   â”‚
â”‚ QPS              â”‚ 151.3  â”‚
â”‚ Build Time (s)   â”‚ 978.09 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/pgvector/random_pgvector_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T01:31:29Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container pgvector_benchmark  Stopping
 Container pgvector_benchmark  Stopped
 Container pgvector_benchmark  Removing
 Container pgvector_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T01:31:34Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container pgvector_benchmark  Creating
 Container pgvector_benchmark  Created
 Container pgvector_benchmark  Starting
 Container pgvector_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: pgvector
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: pgvector on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Pgvector: Inserting 100000 vectors in batches...
   Inserted batch 10000/100000   Inserted batch 20000/100000   Inserted batch 30000/100000   Inserted batch 40000/100000   Inserted batch 50000/100000   Inserted batch 60000/100000   Inserted batch 70000/100000   Inserted batch 80000/100000   Inserted batch 90000/100000   Inserted batch 100000/100000
âœ… Pgvector: Insertion complete.
âš¡ High dimensionality (768d) detected. Boosting HNSW parameters...
ğŸ”¨ Pgvector: Building hnsw index...
    Run 1: Recall@10=0.1000, Latency_p50=8.81ms
    Run 2: Recall@10=0.1000, Latency_p50=8.93ms
    Run 3: Recall@10=0.1000, Latency_p50=8.84ms
  Results: pgvector_msmarco  
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9572 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 8.84   â”‚
â”‚ Latency p99 (ms) â”‚ 11.71  â”‚
â”‚ QPS              â”‚ 112.9  â”‚
â”‚ Build Time (s)   â”‚ 700.10 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/pgvector/msmarco_pgvector_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T01:44:44Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container pgvector_benchmark  Stopping
 Container pgvector_benchmark  Stopped
 Container pgvector_benchmark  Removing
 Container pgvector_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T01:44:50Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container pgvector_benchmark  Creating
 Container pgvector_benchmark  Created
 Container pgvector_benchmark  Starting
 Container pgvector_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: pgvector
Datasets: glove
Runs per config: 3


Loading dataset: glove

Benchmarking: pgvector on glove
  Vectors: (400000, 100), Queries: (10000, 100)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Pgvector: Inserting 400000 vectors in batches...
   Inserted batch 10000/400000   Inserted batch 20000/400000   Inserted batch 30000/400000   Inserted batch 40000/400000   Inserted batch 50000/400000   Inserted batch 60000/400000   Inserted batch 70000/400000   Inserted batch 80000/400000   Inserted batch 90000/400000   Inserted batch 100000/400000   Inserted batch 110000/400000   Inserted batch 120000/400000   Inserted batch 130000/400000   Inserted batch 140000/400000   Inserted batch 150000/400000   Inserted batch 160000/400000   Inserted batch 170000/400000   Inserted batch 180000/400000   Inserted batch 190000/400000   Inserted batch 200000/400000   Inserted batch 210000/400000   Inserted batch 220000/400000   Inserted batch 230000/400000   Inserted batch 240000/400000   Inserted batch 250000/400000   Inserted batch 260000/400000   Inserted batch 270000/400000   Inserted batch 280000/400000   Inserted batch 290000/400000   Inserted batch 300000/400000   Inserted batch 310000/400000   Inserted batch 320000/400000   Inserted batch 330000/400000   Inserted batch 340000/400000   Inserted batch 350000/400000   Inserted batch 360000/400000   Inserted batch 370000/400000   Inserted batch 380000/400000   Inserted batch 390000/400000   Inserted batch 400000/400000
âœ… Pgvector: Insertion complete.
ğŸ”¨ Pgvector: Building hnsw index...
    Run 1: Recall@10=0.0999, Latency_p50=4.25ms
    Run 2: Recall@10=0.0999, Latency_p50=4.09ms
    Run 3: Recall@10=0.0999, Latency_p50=4.23ms
   Results: pgvector_glove   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0999 â”‚
â”‚ Recall@100       â”‚ 0.8649 â”‚
â”‚ MRR              â”‚ 0.9996 â”‚
â”‚ Latency p50 (ms) â”‚ 4.23   â”‚
â”‚ Latency p99 (ms) â”‚ 6.45   â”‚
â”‚ QPS              â”‚ 236.7  â”‚
â”‚ Build Time (s)   â”‚ 300.17 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/pgvector/glove_pgvector_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T02:00:35Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container pgvector_benchmark  Stopping
 Container pgvector_benchmark  Stopped
 Container pgvector_benchmark  Removing
 Container pgvector_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T02:00:36Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container qdrant_benchmark  Creating
 Container qdrant_benchmark  Created
 Container qdrant_benchmark  Starting
 Container qdrant_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: qdrant
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: qdrant on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: HNSW_L2
    Building index...
âš¡ Auto-enabling Scalar Quantization (Int8) for speed...
ğŸš€ Using Optimized Parallel Upload for 1000000 vectors...
â³ Waiting for Qdrant indexing...
   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...
âœ… Optimization complete. Collection is GREEN.
    Run 1: Recall@10=0.1000, Latency_p50=8.86ms
    Run 2: Recall@10=0.1000, Latency_p50=8.90ms
    Run 3: Recall@10=0.1000, Latency_p50=8.74ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: qdrant_gist1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.8355 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 8.74   â”‚
â”‚ Latency p99 (ms) â”‚ 10.05  â”‚
â”‚ QPS              â”‚ 115.0  â”‚
â”‚ Build Time (s)   â”‚ 595.95 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/qdrant/gist1m_qdrant_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T02:12:34Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container qdrant_benchmark  Stopping
 Container qdrant_benchmark  Stopped
 Container qdrant_benchmark  Removing
 Container qdrant_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T02:12:40Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container qdrant_benchmark  Creating
 Container qdrant_benchmark  Created
 Container qdrant_benchmark  Starting
 Container qdrant_benchmark  Started
ğŸ§¹ Cleaning up workspace...

============================================================
 PREPARING BENCHMARK FOR: CHROMA
============================================================
ğŸš€ Starting chroma...
â³ Waiting 30s for chroma...

â–¶ï¸  Running: chroma on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database chroma --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting chroma...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for chroma...

â–¶ï¸  Running: chroma on deep1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database chroma --dataset deep1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting chroma...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for chroma...

â–¶ï¸  Running: chroma on sift1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database chroma --dataset sift1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting chroma...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for chroma...

â–¶ï¸  Running: chroma on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database chroma --dataset random --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting chroma...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for chroma...

â–¶ï¸  Running: chroma on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database chroma --dataset msmarco --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting chroma...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for chroma...

â–¶ï¸  Running: chroma on glove...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database chroma --dataset glove --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping chroma...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: WEAVIATE
============================================================
ğŸš€ Starting weaviate...
â³ Waiting 30s for weaviate...

â–¶ï¸  Running: weaviate on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database weaviate --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting weaviate...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for weaviate...

â–¶ï¸  Running: weaviate on deep1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database weaviate --dataset deep1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting weaviate...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for weaviate...

â–¶ï¸  Running: weaviate on sift1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database weaviate --dataset sift1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting weaviate...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for weaviate...

â–¶ï¸  Running: weaviate on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database weaviate --dataset random --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting weaviate...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for weaviate...

â–¶ï¸  Running: weaviate on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database weaviate --dataset msmarco --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting weaviate...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for weaviate...

â–¶ï¸  Running: weaviate on glove...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database weaviate --dataset glove --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping weaviate...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: PGVECTOR
============================================================
ğŸš€ Starting pgvector...
â³ Waiting 30s for pgvector...

â–¶ï¸  Running: pgvector on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database pgvector --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting pgvector...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for pgvector...

â–¶ï¸  Running: pgvector on deep1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database pgvector --dataset deep1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting pgvector...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for pgvector...

â–¶ï¸  Running: pgvector on sift1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database pgvector --dataset sift1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting pgvector...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for pgvector...

â–¶ï¸  Running: pgvector on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database pgvector --dataset random --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting pgvector...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for pgvector...

â–¶ï¸  Running: pgvector on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database pgvector --dataset msmarco --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting pgvector...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for pgvector...

â–¶ï¸  Running: pgvector on glove...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database pgvector --dataset glove --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping pgvector...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: QDRANT
============================================================
ğŸš€ Starting qdrant...
â³ Waiting 30s for qdrant...

â–¶ï¸  Running: qdrant on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database qdrant --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting qdrant...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for qdrant...

â–¶ï¸  Running: qdrant on deep1m...
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: qdrant
Datasets: deep1m
Runs per config: 3


Loading dataset: deep1m

Benchmarking: qdrant on deep1m
  Vectors: (1000000, 96), Queries: (10000, 96)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Using Optimized Parallel Upload for 1000000 vectors...
â³ Waiting for Qdrant indexing...
   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...
âœ… Optimization complete. Collection is GREEN.
    Run 1: Recall@10=0.1000, Latency_p50=6.01ms
    Run 2: Recall@10=0.1000, Latency_p50=5.93ms
    Run 3: Recall@10=0.1000, Latency_p50=5.89ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: qdrant_deep1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9120 â”‚
â”‚ MRR              â”‚ 0.9999 â”‚
â”‚ Latency p50 (ms) â”‚ 5.89   â”‚
â”‚ Latency p99 (ms) â”‚ 6.98   â”‚
â”‚ QPS              â”‚ 168.9  â”‚
â”‚ Build Time (s)   â”‚ 396.29 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/qdrant/deep1m_qdrant_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T02:23:44Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container qdrant_benchmark  Stopping
 Container qdrant_benchmark  Stopped
 Container qdrant_benchmark  Removing
 Container qdrant_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T02:23:50Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container qdrant_benchmark  Creating
 Container qdrant_benchmark  Created
 Container qdrant_benchmark  Starting
 Container qdrant_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: qdrant
Datasets: sift1m
Runs per config: 3


Loading dataset: sift1m

Benchmarking: qdrant on sift1m
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Using Optimized Parallel Upload for 1000000 vectors...
â³ Waiting for Qdrant indexing...
   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...
âœ… Optimization complete. Collection is GREEN.
    Run 1: Recall@10=0.1000, Latency_p50=5.85ms
    Run 2: Recall@10=0.1000, Latency_p50=5.84ms
    Run 3: Recall@10=0.1000, Latency_p50=5.96ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: qdrant_sift1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9178 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 5.96   â”‚
â”‚ Latency p99 (ms) â”‚ 7.00   â”‚
â”‚ QPS              â”‚ 166.6  â”‚
â”‚ Build Time (s)   â”‚ 405.82 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/qdrant/sift1m_qdrant_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T02:35:02Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container qdrant_benchmark  Stopping
 Container qdrant_benchmark  Stopped
 Container qdrant_benchmark  Removing
 Container qdrant_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T02:35:07Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container qdrant_benchmark  Creating
 Container qdrant_benchmark  Created
 Container qdrant_benchmark  Starting
 Container qdrant_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: qdrant
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: qdrant on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Using Optimized Parallel Upload for 1000000 vectors...
â³ Waiting for Qdrant indexing...
   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...
âœ… Optimization complete. Collection is GREEN.
    Run 1: Recall@10=0.0999, Latency_p50=6.73ms
    Run 2: Recall@10=0.0999, Latency_p50=6.82ms
    Run 3: Recall@10=0.0999, Latency_p50=6.52ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: qdrant_random    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0999 â”‚
â”‚ Recall@100       â”‚ 0.2155 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 6.52   â”‚
â”‚ Latency p99 (ms) â”‚ 7.75   â”‚
â”‚ QPS              â”‚ 152.3  â”‚
â”‚ Build Time (s)   â”‚ 741.59 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/qdrant/random_qdrant_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T02:52:19Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container qdrant_benchmark  Stopping
 Container qdrant_benchmark  Stopped
 Container qdrant_benchmark  Removing
 Container qdrant_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T02:52:24Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container qdrant_benchmark  Creating
 Container qdrant_benchmark  Created
 Container qdrant_benchmark  Starting
 Container qdrant_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: qdrant
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: qdrant on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
âš¡ Auto-enabling Scalar Quantization (Int8) for speed...
ğŸš€ Using Optimized Parallel Upload for 100000 vectors...
â³ Waiting for Qdrant indexing...
   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...
âœ… Optimization complete. Collection is GREEN.
    Run 1: Recall@10=0.1000, Latency_p50=6.92ms
    Run 2: Recall@10=0.1000, Latency_p50=6.84ms
    Run 3: Recall@10=0.1000, Latency_p50=6.95ms
   Results: qdrant_msmarco   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9064 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 6.95   â”‚
â”‚ Latency p99 (ms) â”‚ 8.39   â”‚
â”‚ QPS              â”‚ 143.1  â”‚
â”‚ Build Time (s)   â”‚ 73.40  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/qdrant/msmarco_qdrant_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T02:54:58Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container qdrant_benchmark  Stopping
 Container qdrant_benchmark  Stopped
 Container qdrant_benchmark  Removing
 Container qdrant_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T02:55:04Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container qdrant_benchmark  Creating
 Container qdrant_benchmark  Created
 Container qdrant_benchmark  Starting
 Container qdrant_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: qdrant
Datasets: glove
Runs per config: 3


Loading dataset: glove

Benchmarking: qdrant on glove
  Vectors: (400000, 100), Queries: (10000, 100)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Using Optimized Parallel Upload for 400000 vectors...
â³ Waiting for Qdrant indexing...
   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...
âœ… Optimization complete. Collection is GREEN.
    Run 1: Recall@10=0.0999, Latency_p50=5.86ms
    Run 2: Recall@10=0.0999, Latency_p50=5.83ms
    Run 3: Recall@10=0.0999, Latency_p50=5.59ms
    Results: qdrant_glove    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0999 â”‚
â”‚ Recall@100       â”‚ 0.8446 â”‚
â”‚ MRR              â”‚ 0.9996 â”‚
â”‚ Latency p50 (ms) â”‚ 5.59   â”‚
â”‚ Latency p99 (ms) â”‚ 7.11   â”‚
â”‚ QPS              â”‚ 175.8  â”‚
â”‚ Build Time (s)   â”‚ 228.94 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/qdrant/glove_qdrant_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T03:11:10Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container qdrant_benchmark  Stopping
 Container qdrant_benchmark  Stopped
 Container qdrant_benchmark  Removing
 Container qdrant_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T03:11:11Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container milvus-minio  Creating
 Container milvus-etcd  Creating
 Container milvus-etcd  Created
 Container milvus-minio  Created
 Container milvus-standalone  Creating
 Container milvus-standalone  Created
 Container attu  Creating
 Container attu  Created
 Container milvus-etcd  Starting
 Container milvus-minio  Starting
 Container milvus-minio  Started
 Container milvus-etcd  Started
 Container milvus-standalone  Starting
 Container milvus-standalone  Started
 Container attu  Starting
 Container attu  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: milvus
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: milvus on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Milvus: Inserting 1000000 vectors in batches...
   Inserted batch 0 to 10000   Inserted batch 10000 to 20000   Inserted batch 20000 to 30000   Inserted batch 30000 to 40000   Inserted batch 40000 to 50000   Inserted batch 50000 to 60000   Inserted batch 60000 to 70000   Inserted batch 70000 to 80000   Inserted batch 80000 to 90000   Inserted batch 90000 to 100000   Inserted batch 100000 to 110000   Inserted batch 110000 to 120000   Inserted batch 120000 to 130000   Inserted batch 130000 to 140000   Inserted batch 140000 to 150000   Inserted batch 150000 to 160000   Inserted batch 160000 to 170000   Inserted batch 170000 to 180000   Inserted batch 180000 to 190000   Inserted batch 190000 to 200000   Inserted batch 200000 to 210000   Inserted batch 210000 to 220000   Inserted batch 220000 to 230000   Inserted batch 230000 to 240000   Inserted batch 240000 to 250000   Inserted batch 250000 to 260000   Inserted batch 260000 to 270000   Inserted batch 270000 to 280000   Inserted batch 280000 to 290000   Inserted batch 290000 to 300000   Inserted batch 300000 to 310000   Inserted batch 310000 to 320000   Inserted batch 320000 to 330000   Inserted batch 330000 to 340000   Inserted batch 340000 to 350000   Inserted batch 350000 to 360000   Inserted batch 360000 to 370000   Inserted batch 370000 to 380000   Inserted batch 380000 to 390000   Inserted batch 390000 to 400000   Inserted batch 400000 to 410000   Inserted batch 410000 to 420000   Inserted batch 420000 to 430000   Inserted batch 430000 to 440000   Inserted batch 440000 to 450000   Inserted batch 450000 to 460000   Inserted batch 460000 to 470000   Inserted batch 470000 to 480000   Inserted batch 480000 to 490000   Inserted batch 490000 to 500000   Inserted batch 500000 to 510000   Inserted batch 510000 to 520000   Inserted batch 520000 to 530000   Inserted batch 530000 to 540000   Inserted batch 540000 to 550000   Inserted batch 550000 to 560000   Inserted batch 560000 to 570000   Inserted batch 570000 to 580000   Inserted batch 580000 to 590000   Inserted batch 590000 to 600000   Inserted batch 600000 to 610000   Inserted batch 610000 to 620000   Inserted batch 620000 to 630000   Inserted batch 630000 to 640000   Inserted batch 640000 to 650000   Inserted batch 650000 to 660000   Inserted batch 660000 to 670000   Inserted batch 670000 to 680000   Inserted batch 680000 to 690000   Inserted batch 690000 to 700000   Inserted batch 700000 to 710000   Inserted batch 710000 to 720000   Inserted batch 720000 to 730000   Inserted batch 730000 to 740000   Inserted batch 740000 to 750000   Inserted batch 750000 to 760000   Inserted batch 760000 to 770000   Inserted batch 770000 to 780000   Inserted batch 780000 to 790000   Inserted batch 790000 to 800000   Inserted batch 800000 to 810000   Inserted batch 810000 to 820000   Inserted batch 820000 to 830000   Inserted batch 830000 to 840000   Inserted batch 840000 to 850000   Inserted batch 850000 to 860000   Inserted batch 860000 to 870000   Inserted batch 870000 to 880000   Inserted batch 880000 to 890000   Inserted batch 890000 to 900000   Inserted batch 900000 to 910000   Inserted batch 910000 to 920000   Inserted batch 920000 to 930000   Inserted batch 930000 to 940000   Inserted batch 940000 to 950000   Inserted batch 950000 to 960000   Inserted batch 960000 to 970000   Inserted batch 970000 to 980000   Inserted batch 980000 to 990000   Inserted batch 990000 to 1000000
âœ… Insertion complete.
ğŸ”¨ Milvus: Building index (HNSW)...
    Run 1: Recall@10=0.1000, Latency_p50=38.61ms
    Run 2: Recall@10=0.1000, Latency_p50=38.66ms
    Run 3: Recall@10=0.1000, Latency_p50=37.38ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
    Results: milvus_gist1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000  â”‚
â”‚ Recall@100       â”‚ 0.9743  â”‚
â”‚ MRR              â”‚ 1.0000  â”‚
â”‚ Latency p50 (ms) â”‚ 37.38   â”‚
â”‚ Latency p99 (ms) â”‚ 128.00  â”‚
â”‚ QPS              â”‚ 22.9    â”‚
â”‚ Build Time (s)   â”‚ 1273.42 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/milvus/gist1m_milvus_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T03:36:55Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container attu  Stopping
 Container attu  Stopped
 Container attu  Removing
 Container attu  Removed
 Container milvus-standalone  Stopping
 Container milvus-standalone  Stopped
 Container milvus-standalone  Removing
 Container milvus-standalone  Removed
 Container milvus-etcd  Stopping
 Container milvus-minio  Stopping
 Container milvus-etcd  Stopped
 Container milvus-etcd  Removing
 Container milvus-etcd  Removed
 Container milvus-minio  Stopped
 Container milvus-minio  Removing
 Container milvus-minio  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T03:37:13Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container milvus-etcd  Creating
 Container milvus-minio  Creating
 Container milvus-etcd  Created
 Container milvus-minio  Created
 Container milvus-standalone  Creating
 Container milvus-standalone  Created
 Container attu  Creating
 Container attu  Created
 Container milvus-minio  Starting
 Container milvus-etcd  Starting
 Container milvus-etcd  Started
 Container milvus-minio  Started
 Container milvus-standalone  Starting
 Container milvus-standalone  Started
 Container attu  Starting
 Container attu  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: milvus
Datasets: deep1m
Runs per config: 3


Loading dataset: deep1m

Benchmarking: milvus on deep1m
  Vectors: (1000000, 96), Queries: (10000, 96)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Milvus: Inserting 1000000 vectors in batches...
   Inserted batch 0 to 10000   Inserted batch 10000 to 20000   Inserted batch 20000 to 30000   Inserted batch 30000 to 40000   Inserted batch 40000 to 50000   Inserted batch 50000 to 60000   Inserted batch 60000 to 70000   Inserted batch 70000 to 80000   Inserted batch 80000 to 90000   Inserted batch 90000 to 100000   Inserted batch 100000 to 110000   Inserted batch 110000 to 120000   Inserted batch 120000 to 130000   Inserted batch 130000 to 140000   Inserted batch 140000 to 150000   Inserted batch 150000 to 160000   Inserted batch 160000 to 170000   Inserted batch 170000 to 180000   Inserted batch 180000 to 190000   Inserted batch 190000 to 200000   Inserted batch 200000 to 210000   Inserted batch 210000 to 220000   Inserted batch 220000 to 230000   Inserted batch 230000 to 240000   Inserted batch 240000 to 250000   Inserted batch 250000 to 260000   Inserted batch 260000 to 270000   Inserted batch 270000 to 280000   Inserted batch 280000 to 290000   Inserted batch 290000 to 300000   Inserted batch 300000 to 310000   Inserted batch 310000 to 320000   Inserted batch 320000 to 330000   Inserted batch 330000 to 340000   Inserted batch 340000 to 350000   Inserted batch 350000 to 360000   Inserted batch 360000 to 370000   Inserted batch 370000 to 380000   Inserted batch 380000 to 390000   Inserted batch 390000 to 400000   Inserted batch 400000 to 410000   Inserted batch 410000 to 420000   Inserted batch 420000 to 430000   Inserted batch 430000 to 440000   Inserted batch 440000 to 450000   Inserted batch 450000 to 460000   Inserted batch 460000 to 470000   Inserted batch 470000 to 480000   Inserted batch 480000 to 490000   Inserted batch 490000 to 500000   Inserted batch 500000 to 510000   Inserted batch 510000 to 520000   Inserted batch 520000 to 530000   Inserted batch 530000 to 540000   Inserted batch 540000 to 550000   Inserted batch 550000 to 560000   Inserted batch 560000 to 570000   Inserted batch 570000 to 580000   Inserted batch 580000 to 590000   Inserted batch 590000 to 600000   Inserted batch 600000 to 610000   Inserted batch 610000 to 620000   Inserted batch 620000 to 630000   Inserted batch 630000 to 640000   Inserted batch 640000 to 650000   Inserted batch 650000 to 660000   Inserted batch 660000 to 670000   Inserted batch 670000 to 680000   Inserted batch 680000 to 690000   Inserted batch 690000 to 700000   Inserted batch 700000 to 710000   Inserted batch 710000 to 720000   Inserted batch 720000 to 730000   Inserted batch 730000 to 740000   Inserted batch 740000 to 750000   Inserted batch 750000 to 760000   Inserted batch 760000 to 770000   Inserted batch 770000 to 780000   Inserted batch 780000 to 790000   Inserted batch 790000 to 800000   Inserted batch 800000 to 810000   Inserted batch 810000 to 820000   Inserted batch 820000 to 830000   Inserted batch 830000 to 840000   Inserted batch 840000 to 850000   Inserted batch 850000 to 860000   Inserted batch 860000 to 870000   Inserted batch 870000 to 880000   Inserted batch 880000 to 890000   Inserted batch 890000 to 900000   Inserted batch 900000 to 910000   Inserted batch 910000 to 920000   Inserted batch 920000 to 930000   Inserted batch 930000 to 940000   Inserted batch 940000 to 950000   Inserted batch 950000 to 960000   Inserted batch 960000 to 970000   Inserted batch 970000 to 980000   Inserted batch 980000 to 990000   Inserted batch 990000 to 1000000
âœ… Insertion complete.
ğŸ”¨ Milvus: Building index (HNSW)...
    Run 1: Recall@10=0.1000, Latency_p50=12.67ms
    Run 2: Recall@10=0.1000, Latency_p50=13.00ms
    Run 3: Recall@10=0.1000, Latency_p50=12.27ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: milvus_deep1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9714 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 12.27  â”‚
â”‚ Latency p99 (ms) â”‚ 30.62  â”‚
â”‚ QPS              â”‚ 77.0   â”‚
â”‚ Build Time (s)   â”‚ 291.16 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/milvus/deep1m_milvus_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T03:50:34Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container attu  Stopping
 Container attu  Stopped
 Container attu  Removing
 Container attu  Removed
 Container milvus-standalone  Stopping
 Container milvus-standalone  Stopped
 Container milvus-standalone  Removing
 Container milvus-standalone  Removed
 Container milvus-minio  Stopping
 Container milvus-etcd  Stopping
 Container milvus-etcd  Stopped
 Container milvus-etcd  Removing
 Container milvus-etcd  Removed
 Container milvus-minio  Stopped
 Container milvus-minio  Removing
 Container milvus-minio  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T03:50:52Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container milvus-etcd  Creating
 Container milvus-minio  Creating
 Container milvus-etcd  Created
 Container milvus-minio  Created
 Container milvus-standalone  Creating
 Container milvus-standalone  Created
 Container attu  Creating
 Container attu  Created
 Container milvus-etcd  Starting
 Container milvus-minio  Starting
 Container milvus-minio  Started
 Container milvus-etcd  Started
 Container milvus-standalone  Starting
 Container milvus-standalone  Started
 Container attu  Starting
 Container attu  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: milvus
Datasets: sift1m
Runs per config: 3


Loading dataset: sift1m

Benchmarking: milvus on sift1m
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Milvus: Inserting 1000000 vectors in batches...
   Inserted batch 0 to 10000   Inserted batch 10000 to 20000   Inserted batch 20000 to 30000   Inserted batch 30000 to 40000   Inserted batch 40000 to 50000   Inserted batch 50000 to 60000   Inserted batch 60000 to 70000   Inserted batch 70000 to 80000   Inserted batch 80000 to 90000   Inserted batch 90000 to 100000   Inserted batch 100000 to 110000   Inserted batch 110000 to 120000   Inserted batch 120000 to 130000   Inserted batch 130000 to 140000   Inserted batch 140000 to 150000   Inserted batch 150000 to 160000   Inserted batch 160000 to 170000   Inserted batch 170000 to 180000   Inserted batch 180000 to 190000   Inserted batch 190000 to 200000   Inserted batch 200000 to 210000   Inserted batch 210000 to 220000   Inserted batch 220000 to 230000   Inserted batch 230000 to 240000   Inserted batch 240000 to 250000   Inserted batch 250000 to 260000   Inserted batch 260000 to 270000   Inserted batch 270000 to 280000   Inserted batch 280000 to 290000   Inserted batch 290000 to 300000   Inserted batch 300000 to 310000   Inserted batch 310000 to 320000   Inserted batch 320000 to 330000   Inserted batch 330000 to 340000   Inserted batch 340000 to 350000   Inserted batch 350000 to 360000   Inserted batch 360000 to 370000   Inserted batch 370000 to 380000   Inserted batch 380000 to 390000   Inserted batch 390000 to 400000   Inserted batch 400000 to 410000   Inserted batch 410000 to 420000   Inserted batch 420000 to 430000   Inserted batch 430000 to 440000   Inserted batch 440000 to 450000   Inserted batch 450000 to 460000   Inserted batch 460000 to 470000   Inserted batch 470000 to 480000   Inserted batch 480000 to 490000   Inserted batch 490000 to 500000   Inserted batch 500000 to 510000   Inserted batch 510000 to 520000   Inserted batch 520000 to 530000   Inserted batch 530000 to 540000   Inserted batch 540000 to 550000   Inserted batch 550000 to 560000   Inserted batch 560000 to 570000   Inserted batch 570000 to 580000   Inserted batch 580000 to 590000   Inserted batch 590000 to 600000   Inserted batch 600000 to 610000   Inserted batch 610000 to 620000   Inserted batch 620000 to 630000   Inserted batch 630000 to 640000   Inserted batch 640000 to 650000   Inserted batch 650000 to 660000   Inserted batch 660000 to 670000   Inserted batch 670000 to 680000   Inserted batch 680000 to 690000   Inserted batch 690000 to 700000   Inserted batch 700000 to 710000   Inserted batch 710000 to 720000   Inserted batch 720000 to 730000   Inserted batch 730000 to 740000   Inserted batch 740000 to 750000   Inserted batch 750000 to 760000   Inserted batch 760000 to 770000   Inserted batch 770000 to 780000   Inserted batch 780000 to 790000   Inserted batch 790000 to 800000   Inserted batch 800000 to 810000   Inserted batch 810000 to 820000   Inserted batch 820000 to 830000   Inserted batch 830000 to 840000   Inserted batch 840000 to 850000   Inserted batch 850000 to 860000   Inserted batch 860000 to 870000   Inserted batch 870000 to 880000   Inserted batch 880000 to 890000   Inserted batch 890000 to 900000   Inserted batch 900000 to 910000   Inserted batch 910000 to 920000   Inserted batch 920000 to 930000   Inserted batch 930000 to 940000   Inserted batch 940000 to 950000   Inserted batch 950000 to 960000   Inserted batch 960000 to 970000   Inserted batch 970000 to 980000   Inserted batch 980000 to 990000   Inserted batch 990000 to 1000000
âœ… Insertion complete.
ğŸ”¨ Milvus: Building index (HNSW)...
    Run 1: Recall@10=0.1000, Latency_p50=16.12ms
    Run 2: Recall@10=0.1000, Latency_p50=14.58ms
    Run 3: Recall@10=0.1000, Latency_p50=9.96ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: milvus_sift1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9705 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 9.96   â”‚
â”‚ Latency p99 (ms) â”‚ 21.33  â”‚
â”‚ QPS              â”‚ 94.4   â”‚
â”‚ Build Time (s)   â”‚ 333.18 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/milvus/sift1m_milvus_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T04:05:32Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container attu  Stopping
 Container attu  Stopped
 Container attu  Removing
 Container attu  Removed
 Container milvus-standalone  Stopping
 Container milvus-standalone  Stopped
 Container milvus-standalone  Removing
 Container milvus-standalone  Removed
 Container milvus-etcd  Stopping
 Container milvus-minio  Stopping
 Container milvus-etcd  Stopped
 Container milvus-etcd  Removing
 Container milvus-etcd  Removed
 Container milvus-minio  Stopped
 Container milvus-minio  Removing
 Container milvus-minio  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T04:05:39Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container milvus-minio  Creating
 Container milvus-etcd  Creating
 Container milvus-etcd  Created
 Container milvus-minio  Created
 Container milvus-standalone  Creating
 Container milvus-standalone  Created
 Container attu  Creating
 Container attu  Created
 Container milvus-minio  Starting
 Container milvus-etcd  Starting
 Container milvus-minio  Started
 Container milvus-etcd  Started
 Container milvus-standalone  Starting
 Container milvus-standalone  Started
 Container attu  Starting
 Container attu  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: milvus
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: milvus on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Milvus: Inserting 1000000 vectors in batches...
   Inserted batch 0 to 10000   Inserted batch 10000 to 20000   Inserted batch 20000 to 30000   Inserted batch 30000 to 40000   Inserted batch 40000 to 50000   Inserted batch 50000 to 60000   Inserted batch 60000 to 70000   Inserted batch 70000 to 80000   Inserted batch 80000 to 90000   Inserted batch 90000 to 100000   Inserted batch 100000 to 110000   Inserted batch 110000 to 120000   Inserted batch 120000 to 130000   Inserted batch 130000 to 140000   Inserted batch 140000 to 150000   Inserted batch 150000 to 160000   Inserted batch 160000 to 170000   Inserted batch 170000 to 180000   Inserted batch 180000 to 190000   Inserted batch 190000 to 200000   Inserted batch 200000 to 210000   Inserted batch 210000 to 220000   Inserted batch 220000 to 230000   Inserted batch 230000 to 240000   Inserted batch 240000 to 250000   Inserted batch 250000 to 260000   Inserted batch 260000 to 270000   Inserted batch 270000 to 280000   Inserted batch 280000 to 290000   Inserted batch 290000 to 300000   Inserted batch 300000 to 310000   Inserted batch 310000 to 320000   Inserted batch 320000 to 330000   Inserted batch 330000 to 340000   Inserted batch 340000 to 350000   Inserted batch 350000 to 360000   Inserted batch 360000 to 370000   Inserted batch 370000 to 380000   Inserted batch 380000 to 390000   Inserted batch 390000 to 400000   Inserted batch 400000 to 410000   Inserted batch 410000 to 420000   Inserted batch 420000 to 430000   Inserted batch 430000 to 440000   Inserted batch 440000 to 450000   Inserted batch 450000 to 460000   Inserted batch 460000 to 470000   Inserted batch 470000 to 480000   Inserted batch 480000 to 490000   Inserted batch 490000 to 500000   Inserted batch 500000 to 510000   Inserted batch 510000 to 520000   Inserted batch 520000 to 530000   Inserted batch 530000 to 540000   Inserted batch 540000 to 550000   Inserted batch 550000 to 560000   Inserted batch 560000 to 570000   Inserted batch 570000 to 580000   Inserted batch 580000 to 590000   Inserted batch 590000 to 600000   Inserted batch 600000 to 610000   Inserted batch 610000 to 620000   Inserted batch 620000 to 630000   Inserted batch 630000 to 640000   Inserted batch 640000 to 650000   Inserted batch 650000 to 660000   Inserted batch 660000 to 670000   Inserted batch 670000 to 680000   Inserted batch 680000 to 690000   Inserted batch 690000 to 700000   Inserted batch 700000 to 710000   Inserted batch 710000 to 720000   Inserted batch 720000 to 730000   Inserted batch 730000 to 740000   Inserted batch 740000 to 750000   Inserted batch 750000 to 760000   Inserted batch 760000 to 770000   Inserted batch 770000 to 780000   Inserted batch 780000 to 790000   Inserted batch 790000 to 800000   Inserted batch 800000 to 810000   Inserted batch 810000 to 820000   Inserted batch 820000 to 830000   Inserted batch 830000 to 840000   Inserted batch 840000 to 850000   Inserted batch 850000 to 860000   Inserted batch 860000 to 870000   Inserted batch 870000 to 880000   Inserted batch 880000 to 890000   Inserted batch 890000 to 900000   Inserted batch 900000 to 910000   Inserted batch 910000 to 920000   Inserted batch 920000 to 930000   Inserted batch 930000 to 940000   Inserted batch 940000 to 950000   Inserted batch 950000 to 960000   Inserted batch 960000 to 970000   Inserted batch 970000 to 980000   Inserted batch 980000 to 990000   Inserted batch 990000 to 1000000
âœ… Insertion complete.
ğŸ”¨ Milvus: Building index (HNSW)...
    Run 1: Recall@10=0.1000, Latency_p50=16.23ms
    Run 2: Recall@10=0.1000, Latency_p50=14.76ms
    Run 3: Recall@10=0.1000, Latency_p50=15.03ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: milvus_random    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.4051 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 15.03  â”‚
â”‚ Latency p99 (ms) â”‚ 26.65  â”‚
â”‚ QPS              â”‚ 64.7   â”‚
â”‚ Build Time (s)   â”‚ 679.50 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/milvus/random_milvus_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T04:26:59Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container attu  Stopping
 Container attu  Stopped
 Container attu  Removing
 Container attu  Removed
 Container milvus-standalone  Stopping
 Container milvus-standalone  Stopped
 Container milvus-standalone  Removing
 Container milvus-standalone  Removed
 Container milvus-minio  Stopping
 Container milvus-etcd  Stopping
 Container milvus-etcd  Stopped
 Container milvus-etcd  Removing
 Container milvus-etcd  Removed
 Container milvus-minio  Stopped
 Container milvus-minio  Removing
 Container milvus-minio  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T04:27:16Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container milvus-etcd  Creating
 Container milvus-minio  Creating
 Container milvus-etcd  Created
 Container milvus-minio  Created
 Container milvus-standalone  Creating
 Container milvus-standalone  Created
 Container attu  Creating
 Container attu  Created
 Container milvus-minio  Starting
 Container milvus-etcd  Starting
 Container milvus-minio  Started
 Container milvus-etcd  Started
 Container milvus-standalone  Starting
 Container milvus-standalone  Started
 Container attu  Starting
 Container attu  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: milvus
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: milvus on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Milvus: Inserting 100000 vectors in batches...
   Inserted batch 0 to 10000   Inserted batch 10000 to 20000   Inserted batch 20000 to 30000   Inserted batch 30000 to 40000   Inserted batch 40000 to 50000   Inserted batch 50000 to 60000   Inserted batch 60000 to 70000   Inserted batch 70000 to 80000   Inserted batch 80000 to 90000   Inserted batch 90000 to 100000
âœ… Insertion complete.
ğŸ”¨ Milvus: Building index (HNSW)...
    Run 1: Recall@10=0.1000, Latency_p50=9.42ms
    Run 2: Recall@10=0.1000, Latency_p50=9.61ms
    Run 3: Recall@10=0.1000, Latency_p50=25.39ms
   Results: milvus_msmarco   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9620 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 25.39  â”‚
â”‚ Latency p99 (ms) â”‚ 88.55  â”‚
â”‚ QPS              â”‚ 34.3   â”‚
â”‚ Build Time (s)   â”‚ 123.49 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/milvus/msmarco_milvus_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T04:31:06Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container attu  Stopping
 Container attu  Stopped
 Container attu  Removing
 Container attu  Removed
 Container milvus-standalone  Stopping
 Container milvus-standalone  Stopped
 Container milvus-standalone  Removing
 Container milvus-standalone  Removed
 Container milvus-minio  Stopping
 Container milvus-etcd  Stopping
 Container milvus-etcd  Stopped
 Container milvus-etcd  Removing
 Container milvus-etcd  Removed
 Container milvus-minio  Stopped
 Container milvus-minio  Removing
 Container milvus-minio  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-15T04:31:24Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container milvus-etcd  Creating
 Container milvus-minio  Creating
 Container milvus-etcd  Created
 Container milvus-minio  Created
 Container milvus-standalone  Creating
 Container milvus-standalone  Created
 Container attu  Creating
 Container attu  Created
 Container milvus-minio  Starting
 Container milvus-etcd  Starting
 Container milvus-etcd  Started
 Container milvus-minio  Started
 Container milvus-standalone  Starting
 Container milvus-standalone  Started
 Container attu  Starting
 Container attu  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: milvus
Datasets: glove
Runs per config: 3


Loading dataset: glove

Benchmarking: milvus on glove
  Vectors: (400000, 100), Queries: (10000, 100)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Milvus: Inserting 400000 vectors in batches...
   Inserted batch 0 to 10000   Inserted batch 10000 to 20000   Inserted batch 20000 to 30000   Inserted batch 30000 to 40000   Inserted batch 40000 to 50000   Inserted batch 50000 to 60000   Inserted batch 60000 to 70000   Inserted batch 70000 to 80000   Inserted batch 80000 to 90000   Inserted batch 90000 to 100000   Inserted batch 100000 to 110000   Inserted batch 110000 to 120000   Inserted batch 120000 to 130000   Inserted batch 130000 to 140000   Inserted batch 140000 to 150000   Inserted batch 150000 to 160000   Inserted batch 160000 to 170000   Inserted batch 170000 to 180000   Inserted batch 180000 to 190000   Inserted batch 190000 to 200000   Inserted batch 200000 to 210000   Inserted batch 210000 to 220000   Inserted batch 220000 to 230000   Inserted batch 230000 to 240000   Inserted batch 240000 to 250000   Inserted batch 250000 to 260000   Inserted batch 260000 to 270000   Inserted batch 270000 to 280000   Inserted batch 280000 to 290000   Inserted batch 290000 to 300000   Inserted batch 300000 to 310000   Inserted batch 310000 to 320000   Inserted batch 320000 to 330000   Inserted batch 330000 to 340000   Inserted batch 340000 to 350000   Inserted batch 350000 to 360000   Inserted batch 360000 to 370000   Inserted batch 370000 to 380000   Inserted batch 380000 to 390000   Inserted batch 390000 to 400000
âœ… Insertion complete.
ğŸ”¨ Milvus: Building index (HNSW)...
    Run 1: Recall@10=0.1000, Latency_p50=7.72ms
    Run 2: Recall@10=0.1000, Latency_p50=7.69ms
    Run 3: Recall@10=0.1000, Latency_p50=8.02ms
    Results: milvus_glove    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.8870 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 8.02   â”‚
â”‚ Latency p99 (ms) â”‚ 12.54  â”‚
â”‚ QPS              â”‚ 120.1  â”‚
â”‚ Build Time (s)   â”‚ 194.52 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/milvus/glove_milvus_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-15T04:48:34Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container attu  Stopping
 Container attu  Stopped
 Container attu  Removing
 Container attu  Removed
 Container milvus-standalone  Stopping
 Container milvus-standalone  Stopped
 Container milvus-standalone  Removing
 Container milvus-standalone  Removed
 Container milvus-minio  Stopping
 Container milvus-etcd  Stopping
 Container milvus-etcd  Stopped
 Container milvus-etcd  Removing
 Container milvus-etcd  Removed
 Container milvus-minio  Stopped
 Container milvus-minio  Removing
 Container milvus-minio  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: lancedb
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: lancedb on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: IVF_PQ_L2
    Building index...
ğŸš€ LanceDB: Ingesting 1000000 vectors (PyArrow Table Mode)...
ğŸ”¨ LanceDB: Building IVF-PQ (partitions=256, sub_vectors=16)...
    Run 1: Recall@10=0.0466, Latency_p50=35.87ms
    Run 2: Recall@10=0.0466, Latency_p50=32.64ms
    Run 3: Recall@10=0.0466, Latency_p50=31.00ms
  Skipping IVF_PQ_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: lancedb_gist1m   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0466 â”‚
â”‚ Recall@100       â”‚ 0.2458 â”‚
â”‚ MRR              â”‚ 0.7700 â”‚
â”‚ Latency p50 (ms) â”‚ 31.00  â”‚
â”‚ Latency p99 (ms) â”‚ 40.37  â”‚
â”‚ QPS              â”‚ 31.7   â”‚
â”‚ Build Time (s)   â”‚ 333.37 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/lancedb/gist1m_lancedb_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: lancedb
Datasets: deep1m
Runs per config: 3


Loading dataset: deep1m

Benchmarking: lancedb on deep1m
  Vectors: (1000000, 96), Queries: (10000, 96)
  Metric: l2

  Index: IVF_PQ_L2
    Building index...
ğŸš€ LanceDB: Ingesting 1000000 vectors (PyArrow Table Mode)...
ğŸ”¨ LanceDB: Building IVF-PQ (partitions=256, sub_vectors=16)...
    Run 1: Recall@10=0.0950, Latency_p50=29.51ms
    Run 2: Recall@10=0.0950, Latency_p50=34.24ms
    Run 3: Recall@10=0.0950, Latency_p50=26.76ms
  Skipping IVF_PQ_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: lancedb_deep1m   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0950 â”‚
â”‚ Recall@100       â”‚ 0.6073 â”‚
â”‚ MRR              â”‚ 0.9955 â”‚
â”‚ Latency p50 (ms) â”‚ 26.76  â”‚
â”‚ Latency p99 (ms) â”‚ 36.43  â”‚
â”‚ QPS              â”‚ 36.7   â”‚
â”‚ Build Time (s)   â”‚ 126.02 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/lancedb/deep1m_lancedb_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: lancedb
Datasets: sift1m
Runs per config: 3


Loading dataset: sift1m

Benchmarking: lancedb on sift1m
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: IVF_PQ_L2
    Building index...
ğŸš€ LanceDB: Ingesting 1000000 vectors (PyArrow Table Mode)...
ğŸ”¨ LanceDB: Building IVF-PQ (partitions=256, sub_vectors=16)...
    Run 1: Recall@10=0.0964, Latency_p50=27.70ms
    Run 2: Recall@10=0.0964, Latency_p50=28.21ms
    Run 3: Recall@10=0.0964, Latency_p50=27.56ms
  Skipping IVF_PQ_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: lancedb_sift1m   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0964 â”‚
â”‚ Recall@100       â”‚ 0.6322 â”‚
â”‚ MRR              â”‚ 0.9978 â”‚
â”‚ Latency p50 (ms) â”‚ 27.56  â”‚
â”‚ Latency p99 (ms) â”‚ 42.73  â”‚
â”‚ QPS              â”‚ 35.2   â”‚
â”‚ Build Time (s)   â”‚ 110.73 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/lancedb/sift1m_lancedb_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: lancedb
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: lancedb on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: IVF_PQ_L2
    Building index...
ğŸš€ LanceDB: Ingesting 1000000 vectors (PyArrow Table Mode)...
ğŸ”¨ LanceDB: Building IVF-PQ (partitions=256, sub_vectors=16)...
    Run 1: Recall@10=0.0256, Latency_p50=27.63ms
    Run 2: Recall@10=0.0256, Latency_p50=27.04ms
    Run 3: Recall@10=0.0256, Latency_p50=27.77ms
  Skipping IVF_PQ_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: lancedb_random   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0256 â”‚
â”‚ Recall@100       â”‚ 0.0997 â”‚
â”‚ MRR              â”‚ 0.6002 â”‚
â”‚ Latency p50 (ms) â”‚ 27.77  â”‚
â”‚ Latency p99 (ms) â”‚ 44.65  â”‚
â”‚ QPS              â”‚ 34.8   â”‚
â”‚ Build Time (s)   â”‚ 101.80 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/lancedb/random_lancedb_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: lancedb
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: lancedb on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping IVF_PQ_L2: Dataset requires 'cosine', Config is 'l2'

  Index: IVF_PQ_Cosine
    Building index...
ğŸš€ LanceDB: Ingesting 100000 vectors (PyArrow Table Mode)...
ğŸ”¨ LanceDB: Building IVF-PQ (partitions=256, sub_vectors=16)...
    Run 1: Recall@10=0.0844, Latency_p50=27.30ms
    Run 2: Recall@10=0.0844, Latency_p50=27.79ms
    Run 3: Recall@10=0.0844, Latency_p50=30.44ms
  Results: lancedb_msmarco   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0844 â”‚
â”‚ Recall@100       â”‚ 0.3956 â”‚
â”‚ MRR              â”‚ 0.9801 â”‚
â”‚ Latency p50 (ms) â”‚ 30.44  â”‚
â”‚ Latency p99 (ms) â”‚ 56.08  â”‚
â”‚ QPS              â”‚ 30.5   â”‚
â”‚ Build Time (s)   â”‚ 176.07 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/lancedb/msmarco_lancedb_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: lancedb
Datasets: glove
Runs per config: 3


Loading dataset: glove

Benchmarking: lancedb on glove
  Vectors: (400000, 100), Queries: (10000, 100)
  Metric: cosine
  Skipping IVF_PQ_L2: Dataset requires 'cosine', Config is 'l2'

  Index: IVF_PQ_Cosine
    Building index...
ğŸš€ LanceDB: Ingesting 400000 vectors (PyArrow Table Mode)...
ğŸ”¨ LanceDB: Building IVF-PQ (partitions=256, sub_vectors=10)...
    Run 1: Recall@10=0.0757, Latency_p50=26.95ms
    Run 2: Recall@10=0.0757, Latency_p50=25.47ms
    Run 3: Recall@10=0.0757, Latency_p50=26.43ms
   Results: lancedb_glove    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0757 â”‚
â”‚ Recall@100       â”‚ 0.3912 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 26.43  â”‚
â”‚ Latency p99 (ms) â”‚ 54.38  â”‚
â”‚ QPS              â”‚ 35.2   â”‚
â”‚ Build Time (s)   â”‚ 78.55  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/lancedb/glove_lancedb_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: faiss
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: faiss on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: IVF1024_L2
    Building index...
    Run 1: Recall@10=0.1000, Latency_p50=14.89ms
    Run 2: Recall@10=0.1000, Latency_p50=14.06ms
    Run 3: Recall@10=0.1000, Latency_p50=17.04ms
  Skipping IVF1024_IP: Dataset requires 'l2', Config is 'cosine'
    Results: faiss_gist1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.7794 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 17.04  â”‚
â”‚ Latency p99 (ms) â”‚ 37.31  â”‚
â”‚ QPS              â”‚ 55.9   â”‚
â”‚ Build Time (s)   â”‚ 72.23  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/faiss/gist1m_faiss_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: faiss
Datasets: deep1m
Runs per config: 3


Loading dataset: deep1m

Benchmarking: faiss on deep1m
  Vectors: (1000000, 96), Queries: (10000, 96)
  Metric: l2

  Index: IVF1024_L2
    Building index...
    Run 1: Recall@10=0.1000, Latency_p50=1.20ms
    Run 2: Recall@10=0.1000, Latency_p50=1.18ms
    Run 3: Recall@10=0.1000, Latency_p50=1.22ms
  Skipping IVF1024_IP: Dataset requires 'l2', Config is 'cosine'
    Results: faiss_deep1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9191 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 1.22   â”‚
â”‚ Latency p99 (ms) â”‚ 3.17   â”‚
â”‚ QPS              â”‚ 748.8  â”‚
â”‚ Build Time (s)   â”‚ 13.88  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/faiss/deep1m_faiss_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: faiss
Datasets: sift1m
Runs per config: 3


Loading dataset: sift1m

Benchmarking: faiss on sift1m
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: IVF1024_L2
    Building index...
    Run 1: Recall@10=0.1000, Latency_p50=1.55ms
    Run 2: Recall@10=0.1000, Latency_p50=1.76ms
    Run 3: Recall@10=0.1000, Latency_p50=1.56ms
  Skipping IVF1024_IP: Dataset requires 'l2', Config is 'cosine'
    Results: faiss_sift1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.9058 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 1.56   â”‚
â”‚ Latency p99 (ms) â”‚ 3.07   â”‚
â”‚ QPS              â”‚ 610.2  â”‚
â”‚ Build Time (s)   â”‚ 14.96  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/faiss/sift1m_faiss_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: faiss
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: faiss on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: IVF1024_L2
    Building index...
    Run 1: Recall@10=0.0990, Latency_p50=1.90ms
    Run 2: Recall@10=0.0987, Latency_p50=1.88ms
    Run 3: Recall@10=0.0982, Latency_p50=1.93ms
  Skipping IVF1024_IP: Dataset requires 'l2', Config is 'cosine'
    Results: faiss_random    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0982 â”‚
â”‚ Recall@100       â”‚ 0.1536 â”‚
â”‚ MRR              â”‚ 0.9928 â”‚
â”‚ Latency p50 (ms) â”‚ 1.93   â”‚
â”‚ Latency p99 (ms) â”‚ 4.29   â”‚
â”‚ QPS              â”‚ 477.1  â”‚
â”‚ Build Time (s)   â”‚ 13.82  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/faiss/random_faiss_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: faiss
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: faiss on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping IVF1024_L2: Dataset requires 'cosine', Config is 'l2'

  Index: IVF1024_IP
    Building index...
WARNING clustering 10000 points to 1024 centroids: please provide at least 39936 training points
    Run 1: Recall@10=0.1000, Latency_p50=1.08ms
    Run 2: Recall@10=0.1000, Latency_p50=1.11ms
    Run 3: Recall@10=0.1000, Latency_p50=1.17ms
   Results: faiss_msmarco    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.6650 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 1.17   â”‚
â”‚ Latency p99 (ms) â”‚ 3.79   â”‚
â”‚ QPS              â”‚ 740.0  â”‚
â”‚ Build Time (s)   â”‚ 6.98   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/faiss/msmarco_faiss_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: faiss
Datasets: glove
Runs per config: 3


Loading dataset: glove

Benchmarking: faiss on glove
  Vectors: (400000, 100), Queries: (10000, 100)
  Metric: cosine
  Skipping IVF1024_L2: Dataset requires 'cosine', Config is 'l2'

  Index: IVF1024_IP
    Building index...
    Run 1: Recall@10=0.1000, Latency_p50=0.72ms
    Run 2: Recall@10=0.1000, Latency_p50=0.72ms
    Run 3: Recall@10=0.1000, Latency_p50=0.72ms
    Results: faiss_glove     
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.8505 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 0.72   â”‚
â”‚ Latency p99 (ms) â”‚ 1.44   â”‚
â”‚ QPS              â”‚ 1325.5 â”‚
â”‚ Build Time (s)   â”‚ 4.93   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/faiss/glove_faiss_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database qdrant --dataset deep1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting qdrant...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for qdrant...

â–¶ï¸  Running: qdrant on sift1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database qdrant --dataset sift1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting qdrant...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for qdrant...

â–¶ï¸  Running: qdrant on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database qdrant --dataset random --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting qdrant...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for qdrant...

â–¶ï¸  Running: qdrant on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database qdrant --dataset msmarco --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting qdrant...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for qdrant...

â–¶ï¸  Running: qdrant on glove...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database qdrant --dataset glove --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping qdrant...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: MILVUS
============================================================
ğŸš€ Starting milvus...
â³ Polling Milvus port 19530...
âœ… Milvus is ready!

â–¶ï¸  Running: milvus on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database milvus --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting milvus...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Polling Milvus port 19530...
âœ… Milvus is ready!

â–¶ï¸  Running: milvus on deep1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database milvus --dataset deep1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting milvus...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Polling Milvus port 19530...
âœ… Milvus is ready!

â–¶ï¸  Running: milvus on sift1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database milvus --dataset sift1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting milvus...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Polling Milvus port 19530...
âœ… Milvus is ready!

â–¶ï¸  Running: milvus on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database milvus --dataset random --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting milvus...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Polling Milvus port 19530...
âœ… Milvus is ready!

â–¶ï¸  Running: milvus on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database milvus --dataset msmarco --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting milvus...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Polling Milvus port 19530...
âœ… Milvus is ready!

â–¶ï¸  Running: milvus on glove...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database milvus --dataset glove --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping milvus...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: LANCEDB
============================================================

â–¶ï¸  Running: lancedb on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database lancedb --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â„ï¸  Cooling down...

â–¶ï¸  Running: lancedb on deep1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database lancedb --dataset deep1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â„ï¸  Cooling down...

â–¶ï¸  Running: lancedb on sift1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database lancedb --dataset sift1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â„ï¸  Cooling down...

â–¶ï¸  Running: lancedb on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database lancedb --dataset random --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â„ï¸  Cooling down...

â–¶ï¸  Running: lancedb on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database lancedb --dataset msmarco --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â„ï¸  Cooling down...

â–¶ï¸  Running: lancedb on glove...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database lancedb --dataset glove --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.

============================================================
 PREPARING BENCHMARK FOR: FAISS
============================================================

â–¶ï¸  Running: faiss on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database faiss --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â„ï¸  Cooling down...

â–¶ï¸  Running: faiss on deep1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database faiss --dataset deep1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â„ï¸  Cooling down...

â–¶ï¸  Running: faiss on sift1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database faiss --dataset sift1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â„ï¸  Cooling down...

â–¶ï¸  Running: faiss on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database faiss --dataset random --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â„ï¸  Cooling down...

â–¶ï¸  Running: faiss on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database faiss --dataset msmarco --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
â„ï¸  Cooling down...

â–¶ï¸  Running: faiss on glove...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database faiss --dataset glove --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
