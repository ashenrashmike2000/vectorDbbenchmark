time="2026-01-14T03:11:10Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-14T03:11:11Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-14T03:11:11Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-14T03:11:11Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-14T03:11:11Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-14T03:11:11Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container chroma-server  Creating
 Container chroma-server  Created
 Container chroma-server  Starting
 Container chroma-server  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: chroma
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: chroma on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Chroma: Inserting 1000000 vectors in batches...
   Processed 2000/1000000 vectors...   Processed 12000/1000000 vectors...   Processed 22000/1000000 vectors...   Processed 32000/1000000 vectors...   Processed 42000/1000000 vectors...   Processed 52000/1000000 vectors...   Processed 62000/1000000 vectors...   Processed 72000/1000000 vectors...   Processed 82000/1000000 vectors...   Processed 92000/1000000 vectors...   Processed 102000/1000000 vectors...   Processed 112000/1000000 vectors...   Processed 122000/1000000 vectors...   Processed 132000/1000000 vectors...   Processed 142000/1000000 vectors...   Processed 152000/1000000 vectors...   Processed 162000/1000000 vectors...   Processed 172000/1000000 vectors...   Processed 182000/1000000 vectors...   Processed 192000/1000000 vectors...   Processed 202000/1000000 vectors...   Processed 212000/1000000 vectors...   Processed 222000/1000000 vectors...   Processed 232000/1000000 vectors...   Processed 242000/1000000 vectors...   Processed 252000/1000000 vectors...   Processed 262000/1000000 vectors...   Processed 272000/1000000 vectors...   Processed 282000/1000000 vectors...   Processed 292000/1000000 vectors...   Processed 302000/1000000 vectors...   Processed 312000/1000000 vectors...   Processed 322000/1000000 vectors...   Processed 332000/1000000 vectors...   Processed 342000/1000000 vectors...   Processed 352000/1000000 vectors...   Processed 362000/1000000 vectors...   Processed 372000/1000000 vectors...   Processed 382000/1000000 vectors...   Processed 392000/1000000 vectors...   Processed 402000/1000000 vectors...   Processed 412000/1000000 vectors...   Processed 422000/1000000 vectors...   Processed 432000/1000000 vectors...   Processed 442000/1000000 vectors...   Processed 452000/1000000 vectors...   Processed 462000/1000000 vectors...   Processed 472000/1000000 vectors...   Processed 482000/1000000 vectors...   Processed 492000/1000000 vectors...   Processed 502000/1000000 vectors...   Processed 512000/1000000 vectors...   Processed 522000/1000000 vectors...   Processed 532000/1000000 vectors...   Processed 542000/1000000 vectors...   Processed 552000/1000000 vectors...   Processed 562000/1000000 vectors...   Processed 572000/1000000 vectors...   Processed 582000/1000000 vectors...   Processed 592000/1000000 vectors...   Processed 602000/1000000 vectors...   Processed 612000/1000000 vectors...   Processed 622000/1000000 vectors...   Processed 632000/1000000 vectors...   Processed 642000/1000000 vectors...   Processed 652000/1000000 vectors...   Processed 662000/1000000 vectors...   Processed 672000/1000000 vectors...   Processed 682000/1000000 vectors...   Processed 692000/1000000 vectors...   Processed 702000/1000000 vectors...   Processed 712000/1000000 vectors...   Processed 722000/1000000 vectors...   Processed 732000/1000000 vectors...   Processed 742000/1000000 vectors...   Processed 752000/1000000 vectors...   Processed 762000/1000000 vectors...   Processed 772000/1000000 vectors...   Processed 782000/1000000 vectors...   Processed 792000/1000000 vectors...   Processed 802000/1000000 vectors...   Processed 812000/1000000 vectors...   Processed 822000/1000000 vectors...   Processed 832000/1000000 vectors...   Processed 842000/1000000 vectors...   Processed 852000/1000000 vectors...   Processed 862000/1000000 vectors...   Processed 872000/1000000 vectors...   Processed 882000/1000000 vectors...   Processed 892000/1000000 vectors...   Processed 902000/1000000 vectors...   Processed 912000/1000000 vectors...   Processed 922000/1000000 vectors...   Processed 932000/1000000 vectors...   Processed 942000/1000000 vectors...   Processed 952000/1000000 vectors...   Processed 962000/1000000 vectors...   Processed 972000/1000000 vectors...   Processed 982000/1000000 vectors...   Processed 992000/1000000 vectors...
âœ… Chroma: Insertion complete.
    Run 1: Recall@10=0.1000, Latency_p50=8.68ms
    Run 2: Recall@10=0.1000, Latency_p50=9.13ms
    Run 3: Recall@10=0.1000, Latency_p50=8.87ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
    Results: chroma_gist1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000  â”‚
â”‚ Recall@100       â”‚ 0.6650  â”‚
â”‚ MRR              â”‚ 1.0000  â”‚
â”‚ Latency p50 (ms) â”‚ 8.87    â”‚
â”‚ Latency p99 (ms) â”‚ 11.49   â”‚
â”‚ QPS              â”‚ 111.7   â”‚
â”‚ Build Time (s)   â”‚ 1824.27 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/chroma/gist1m_chroma_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T03:43:45Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container chroma-server  Stopping
 Container chroma-server  Stopped
 Container chroma-server  Removing
 Container chroma-server  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T03:43:47Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container ragdbeval-weaviate-1  Creating
 Container ragdbeval-weaviate-1  Created
 Container ragdbeval-weaviate-1  Starting
 Container ragdbeval-weaviate-1  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: weaviate
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: weaviate on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Weaviate: Inserting 1000000 vectors (Fixed Batch Mode)...
   Processed 10000 vectors...   Processed 20000 vectors...   Processed 30000 vectors...   Processed 40000 vectors...   Processed 50000 vectors...   Processed 60000 vectors...   Processed 70000 vectors...   Processed 80000 vectors...   Processed 90000 vectors...   Processed 100000 vectors...   Processed 110000 vectors...   Processed 120000 vectors...   Processed 130000 vectors...   Processed 140000 vectors...   Processed 150000 vectors...   Processed 160000 vectors...   Processed 170000 vectors...   Processed 180000 vectors...   Processed 190000 vectors...   Processed 200000 vectors...   Processed 210000 vectors...   Processed 220000 vectors...   Processed 230000 vectors...   Processed 240000 vectors...   Processed 250000 vectors...   Processed 260000 vectors...   Processed 270000 vectors...   Processed 280000 vectors...   Processed 290000 vectors...   Processed 300000 vectors...   Processed 310000 vectors...   Processed 320000 vectors...   Processed 330000 vectors...   Processed 340000 vectors...   Processed 350000 vectors...   Processed 360000 vectors...   Processed 370000 vectors...   Processed 380000 vectors...   Processed 390000 vectors...   Processed 400000 vectors...   Processed 410000 vectors...   Processed 420000 vectors...   Processed 430000 vectors...   Processed 440000 vectors...   Processed 450000 vectors...   Processed 460000 vectors...   Processed 470000 vectors...   Processed 480000 vectors...   Processed 490000 vectors...   Processed 500000 vectors...   Processed 510000 vectors...   Processed 520000 vectors...   Processed 530000 vectors...   Processed 540000 vectors...   Processed 550000 vectors...   Processed 560000 vectors...   Processed 570000 vectors...   Processed 580000 vectors...   Processed 590000 vectors...   Processed 600000 vectors...   Processed 610000 vectors...   Processed 620000 vectors...   Processed 630000 vectors...   Processed 640000 vectors...   Processed 650000 vectors...   Processed 660000 vectors...   Processed 670000 vectors...   Processed 680000 vectors...   Processed 690000 vectors...   Processed 700000 vectors...   Processed 710000 vectors...   Processed 720000 vectors...   Processed 730000 vectors...   Processed 740000 vectors...   Processed 750000 vectors...   Processed 760000 vectors...   Processed 770000 vectors...   Processed 780000 vectors...   Processed 790000 vectors...   Processed 800000 vectors...   Processed 810000 vectors...   Processed 820000 vectors...   Processed 830000 vectors...   Processed 840000 vectors...   Processed 850000 vectors...   Processed 860000 vectors...   Processed 870000 vectors...   Processed 880000 vectors...   Processed 890000 vectors...   Processed 900000 vectors...   Processed 910000 vectors...   Processed 920000 vectors...   Processed 930000 vectors...   Processed 940000 vectors...   Processed 950000 vectors...   Processed 960000 vectors...   Processed 970000 vectors...   Processed 980000 vectors...   Processed 990000 vectors...
â³ Weaviate: Waiting for indexing (Shards READY)...
âœ… All shards READY.
    Run 1: Recall@10=0.1000, Latency_p50=37.97ms
    Run 2: Recall@10=0.1000, Latency_p50=36.27ms
    Run 3: Recall@10=0.1000, Latency_p50=38.41ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: weaviate_gist1m   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000  â”‚
â”‚ Recall@100       â”‚ 0.9096  â”‚
â”‚ MRR              â”‚ 1.0000  â”‚
â”‚ Latency p50 (ms) â”‚ 38.41   â”‚
â”‚ Latency p99 (ms) â”‚ 61.65   â”‚
â”‚ QPS              â”‚ 25.6    â”‚
â”‚ Build Time (s)   â”‚ 1371.16 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/weaviate/gist1m_weaviate_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T04:10:30Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container ragdbeval-weaviate-1  Stopping
 Container ragdbeval-weaviate-1  Stopped
 Container ragdbeval-weaviate-1  Removing
 Container ragdbeval-weaviate-1  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T04:10:33Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container pgvector_benchmark  Creating
 Container pgvector_benchmark  Created
 Container pgvector_benchmark  Starting
 Container pgvector_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: pgvector
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: pgvector on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Pgvector: Inserting 1000000 vectors in batches...
   Inserted batch 10000/1000000   Inserted batch 20000/1000000   Inserted batch 30000/1000000   Inserted batch 40000/1000000   Inserted batch 50000/1000000   Inserted batch 60000/1000000   Inserted batch 70000/1000000   Inserted batch 80000/1000000   Inserted batch 90000/1000000   Inserted batch 100000/1000000   Inserted batch 110000/1000000   Inserted batch 120000/1000000   Inserted batch 130000/1000000   Inserted batch 140000/1000000   Inserted batch 150000/1000000   Inserted batch 160000/1000000   Inserted batch 170000/1000000   Inserted batch 180000/1000000   Inserted batch 190000/1000000   Inserted batch 200000/1000000   Inserted batch 210000/1000000   Inserted batch 220000/1000000   Inserted batch 230000/1000000   Inserted batch 240000/1000000   Inserted batch 250000/1000000   Inserted batch 260000/1000000   Inserted batch 270000/1000000   Inserted batch 280000/1000000   Inserted batch 290000/1000000   Inserted batch 300000/1000000   Inserted batch 310000/1000000   Inserted batch 320000/1000000   Inserted batch 330000/1000000   Inserted batch 340000/1000000   Inserted batch 350000/1000000   Inserted batch 360000/1000000   Inserted batch 370000/1000000   Inserted batch 380000/1000000   Inserted batch 390000/1000000   Inserted batch 400000/1000000   Inserted batch 410000/1000000   Inserted batch 420000/1000000   Inserted batch 430000/1000000   Inserted batch 440000/1000000   Inserted batch 450000/1000000   Inserted batch 460000/1000000   Inserted batch 470000/1000000   Inserted batch 480000/1000000   Inserted batch 490000/1000000   Inserted batch 500000/1000000   Inserted batch 510000/1000000   Inserted batch 520000/1000000   Inserted batch 530000/1000000   Inserted batch 540000/1000000   Inserted batch 550000/1000000   Inserted batch 560000/1000000   Inserted batch 570000/1000000   Inserted batch 580000/1000000   Inserted batch 590000/1000000   Inserted batch 600000/1000000   Inserted batch 610000/1000000   Inserted batch 620000/1000000   Inserted batch 630000/1000000   Inserted batch 640000/1000000   Inserted batch 650000/1000000   Inserted batch 660000/1000000   Inserted batch 670000/1000000   Inserted batch 680000/1000000   Inserted batch 690000/1000000   Inserted batch 700000/1000000   Inserted batch 710000/1000000   Inserted batch 720000/1000000   Inserted batch 730000/1000000   Inserted batch 740000/1000000   Inserted batch 750000/1000000   Inserted batch 760000/1000000   Inserted batch 770000/1000000   Inserted batch 780000/1000000   Inserted batch 790000/1000000   Inserted batch 800000/1000000   Inserted batch 810000/1000000   Inserted batch 820000/1000000   Inserted batch 830000/1000000   Inserted batch 840000/1000000   Inserted batch 850000/1000000   Inserted batch 860000/1000000   Inserted batch 870000/1000000   Inserted batch 880000/1000000   Inserted batch 890000/1000000   Inserted batch 900000/1000000   Inserted batch 910000/1000000   Inserted batch 920000/1000000   Inserted batch 930000/1000000   Inserted batch 940000/1000000   Inserted batch 950000/1000000   Inserted batch 960000/1000000   Inserted batch 970000/1000000   Inserted batch 980000/1000000   Inserted batch 990000/1000000   Inserted batch 1000000/1000000
âœ… Pgvector: Insertion complete.
âš¡ High dimensionality (960d) detected. Boosting HNSW parameters...
ğŸ”¨ Pgvector: Building hnsw index...
    Run 1: Recall@10=0.1000, Latency_p50=168.73ms
    Run 2: Recall@10=0.1000, Latency_p50=49.28ms
    Run 3: Recall@10=0.1000, Latency_p50=34.93ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: pgvector_gist1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value    â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000   â”‚
â”‚ Recall@100       â”‚ 0.8738   â”‚
â”‚ MRR              â”‚ 1.0000   â”‚
â”‚ Latency p50 (ms) â”‚ 34.93    â”‚
â”‚ Latency p99 (ms) â”‚ 188.76   â”‚
â”‚ QPS              â”‚ 23.8     â”‚
â”‚ Build Time (s)   â”‚ 20333.30 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/pgvector/gist1m_pgvector_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T09:58:34Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container pgvector_benchmark  Stopping
 Container pgvector_benchmark  Stopped
 Container pgvector_benchmark  Removing
 Container pgvector_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T09:58:36Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container qdrant_benchmark  Creating
 Container qdrant_benchmark  Created
 Container qdrant_benchmark  Starting
 Container qdrant_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: qdrant
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: qdrant on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: HNSW_L2
    Building index...
âš¡ Auto-enabling Scalar Quantization (Int8) for speed...
ğŸš€ Using Optimized Parallel Upload for 1000000 vectors...
â³ Waiting for Qdrant indexing...
   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...
âœ… Optimization complete. Collection is GREEN.
    Run 1: Recall@10=0.1000, Latency_p50=8.92ms
    Run 2: Recall@10=0.1000, Latency_p50=8.82ms
    Run 3: Recall@10=0.1000, Latency_p50=8.99ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: qdrant_gist1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.8569 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 8.99   â”‚
â”‚ Latency p99 (ms) â”‚ 12.03  â”‚
â”‚ QPS              â”‚ 110.4  â”‚
â”‚ Build Time (s)   â”‚ 560.04 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/qdrant/gist1m_qdrant_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T10:10:01Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container qdrant_benchmark  Stopping
 Container qdrant_benchmark  Stopped
 Container qdrant_benchmark  Removing
 Container qdrant_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-14T10:10:02Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container milvus-minio  Creating
 Container milvus-etcd  Creating
 Container milvus-etcd  Created
 Container milvus-minio  Created
 Container milvus-standalone  Creating
 Container milvus-standalone  Created
 Container attu  Creating
 Container attu  Created
 Container milvus-minio  Starting
 Container milvus-etcd  Starting
 Container milvus-etcd  Started
 Container milvus-minio  Started
 Container milvus-standalone  Starting
 Container milvus-standalone  Started
 Container attu  Starting
 Container attu  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: milvus
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: milvus on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Milvus: Inserting 1000000 vectors in batches...
   Inserted batch 0 to 10000   Inserted batch 10000 to 20000   Inserted batch 20000 to 30000   Inserted batch 30000 to 40000   Inserted batch 40000 to 50000   Inserted batch 50000 to 60000   Inserted batch 60000 to 70000   Inserted batch 70000 to 80000   Inserted batch 80000 to 90000   Inserted batch 90000 to 100000   Inserted batch 100000 to 110000   Inserted batch 110000 to 120000   Inserted batch 120000 to 130000   Inserted batch 130000 to 140000   Inserted batch 140000 to 150000   Inserted batch 150000 to 160000   Inserted batch 160000 to 170000   Inserted batch 170000 to 180000   Inserted batch 180000 to 190000   Inserted batch 190000 to 200000   Inserted batch 200000 to 210000   Inserted batch 210000 to 220000   Inserted batch 220000 to 230000   Inserted batch 230000 to 240000   Inserted batch 240000 to 250000   Inserted batch 250000 to 260000   Inserted batch 260000 to 270000   Inserted batch 270000 to 280000   Inserted batch 280000 to 290000   Inserted batch 290000 to 300000   Inserted batch 300000 to 310000   Inserted batch 310000 to 320000   Inserted batch 320000 to 330000   Inserted batch 330000 to 340000   Inserted batch 340000 to 350000   Inserted batch 350000 to 360000   Inserted batch 360000 to 370000   Inserted batch 370000 to 380000   Inserted batch 380000 to 390000   Inserted batch 390000 to 400000   Inserted batch 400000 to 410000   Inserted batch 410000 to 420000   Inserted batch 420000 to 430000   Inserted batch 430000 to 440000   Inserted batch 440000 to 450000   Inserted batch 450000 to 460000   Inserted batch 460000 to 470000   Inserted batch 470000 to 480000   Inserted batch 480000 to 490000   Inserted batch 490000 to 500000   Inserted batch 500000 to 510000   Inserted batch 510000 to 520000   Inserted batch 520000 to 530000   Inserted batch 530000 to 540000   Inserted batch 540000 to 550000   Inserted batch 550000 to 560000   Inserted batch 560000 to 570000   Inserted batch 570000 to 580000   Inserted batch 580000 to 590000   Inserted batch 590000 to 600000   Inserted batch 600000 to 610000   Inserted batch 610000 to 620000   Inserted batch 620000 to 630000   Inserted batch 630000 to 640000   Inserted batch 640000 to 650000   Inserted batch 650000 to 660000   Inserted batch 660000 to 670000   Inserted batch 670000 to 680000   Inserted batch 680000 to 690000   Inserted batch 690000 to 700000   Inserted batch 700000 to 710000   Inserted batch 710000 to 720000   Inserted batch 720000 to 730000   Inserted batch 730000 to 740000   Inserted batch 740000 to 750000   Inserted batch 750000 to 760000   Inserted batch 760000 to 770000   Inserted batch 770000 to 780000   Inserted batch 780000 to 790000   Inserted batch 790000 to 800000   Inserted batch 800000 to 810000   Inserted batch 810000 to 820000   Inserted batch 820000 to 830000   Inserted batch 830000 to 840000   Inserted batch 840000 to 850000   Inserted batch 850000 to 860000   Inserted batch 860000 to 870000   Inserted batch 870000 to 880000   Inserted batch 880000 to 890000   Inserted batch 890000 to 900000   Inserted batch 900000 to 910000   Inserted batch 910000 to 920000   Inserted batch 920000 to 930000   Inserted batch 930000 to 940000   Inserted batch 940000 to 950000   Inserted batch 950000 to 960000   Inserted batch 960000 to 970000   Inserted batch 970000 to 980000   Inserted batch 980000 to 990000   Inserted batch 990000 to 1000000
âœ… Insertion complete.
ğŸ”¨ Milvus: Building index (HNSW)...
    Run 1: Recall@10=0.1000, Latency_p50=37.91ms
    Run 2: Recall@10=0.1000, Latency_p50=39.11ms
    Run 3: Recall@10=0.1000, Latency_p50=39.20ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
    Results: milvus_gist1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value   â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000  â”‚
â”‚ Recall@100       â”‚ 0.9743  â”‚
â”‚ MRR              â”‚ 1.0000  â”‚
â”‚ Latency p50 (ms) â”‚ 39.20   â”‚
â”‚ Latency p99 (ms) â”‚ 74.94   â”‚
â”‚ QPS              â”‚ 24.5    â”‚
â”‚ Build Time (s)   â”‚ 1155.71 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/milvus/gist1m_milvus_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
time="2026-01-14T10:33:33Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container attu  Stopping
 Container attu  Stopped
 Container attu  Removing
 Container attu  Removed
 Container milvus-standalone  Stopping
 Container milvus-standalone  Stopped
 Container milvus-standalone  Removing
 Container milvus-standalone  Removed
 Container milvus-etcd  Stopping
 Container milvus-minio  Stopping
 Container milvus-etcd  Stopped
 Container milvus-etcd  Removing
 Container milvus-etcd  Removed
 Container milvus-minio  Stopped
 Container milvus-minio  Removing
 Container milvus-minio  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: lancedb
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: lancedb on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: IVF_PQ_L2
    Building index...
ğŸš€ LanceDB: Ingesting 1000000 vectors (PyArrow Table Mode)...
ğŸ”¨ LanceDB: Building IVF-PQ (partitions=256, sub_vectors=16)...
    Run 1: Recall@10=0.0472, Latency_p50=28.34ms
    Run 2: Recall@10=0.0472, Latency_p50=29.15ms
    Run 3: Recall@10=0.0472, Latency_p50=28.99ms
  Skipping IVF_PQ_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: lancedb_gist1m   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0472 â”‚
â”‚ Recall@100       â”‚ 0.2446 â”‚
â”‚ MRR              â”‚ 0.7733 â”‚
â”‚ Latency p50 (ms) â”‚ 28.99  â”‚
â”‚ Latency p99 (ms) â”‚ 39.16  â”‚
â”‚ QPS              â”‚ 33.9   â”‚
â”‚ Build Time (s)   â”‚ 308.28 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/lancedb/gist1m_lancedb_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: faiss
Datasets: gist1m
Runs per config: 3


Loading dataset: gist1m

Benchmarking: faiss on gist1m
  Vectors: (1000000, 960), Queries: (1000, 960)
  Metric: l2

  Index: IVF1024_L2
    Building index...
    Run 1: Recall@10=0.1000, Latency_p50=13.34ms
    Run 2: Recall@10=0.1000, Latency_p50=17.29ms
    Run 3: Recall@10=0.1000, Latency_p50=13.63ms
  Skipping IVF1024_IP: Dataset requires 'l2', Config is 'cosine'
    Results: faiss_gist1m    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Recall@100       â”‚ 0.7830 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 13.63  â”‚
â”‚ Latency p99 (ms) â”‚ 22.12  â”‚
â”‚ QPS              â”‚ 71.4   â”‚
â”‚ Build Time (s)   â”‚ 74.10  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Exported JSON to results/faiss/gist1m_faiss_results.json
Exported CSV to results/results.csv
Exported LaTeX tables to results/tables
Generated plots in results/plots

Benchmark complete!
ğŸ§¹ Cleaning up workspace...

============================================================
 PREPARING BENCHMARK FOR: CHROMA
============================================================
ğŸš€ Starting chroma...
â³ Waiting 30s for chroma...

â–¶ï¸  Running: chroma on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database chroma --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping chroma...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: WEAVIATE
============================================================
ğŸš€ Starting weaviate...
â³ Waiting 30s for weaviate...

â–¶ï¸  Running: weaviate on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database weaviate --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping weaviate...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: PGVECTOR
============================================================
ğŸš€ Starting pgvector...
â³ Waiting 30s for pgvector...

â–¶ï¸  Running: pgvector on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database pgvector --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping pgvector...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: QDRANT
============================================================
ğŸš€ Starting qdrant...
â³ Waiting 30s for qdrant...

â–¶ï¸  Running: qdrant on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database qdrant --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping qdrant...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: MILVUS
============================================================
ğŸš€ Starting milvus...
â³ Polling Milvus port 19530...
âœ… Milvus is ready!

â–¶ï¸  Running: milvus on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database milvus --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping milvus...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: LANCEDB
============================================================

â–¶ï¸  Running: lancedb on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database lancedb --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.

============================================================
 PREPARING BENCHMARK FOR: FAISS
============================================================

â–¶ï¸  Running: faiss on gist1m...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database faiss --dataset gist1m --runs 3 --export json csv latex plots
âœ… Benchmark Run Successful.
