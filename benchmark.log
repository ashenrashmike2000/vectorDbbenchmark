time="2026-01-17T09:54:50Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-17T09:54:50Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-17T09:54:50Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/pgvector-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-17T09:54:50Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-17T09:54:50Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
time="2026-01-17T09:54:50Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container chroma-server  Creating
 Container chroma-server  Created
 Container chroma-server  Starting
 Container chroma-server  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: chroma
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: chroma on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Chroma: Inserting 1000000 vectors in batches...
   Processed 2000/1000000 vectors...   Processed 12000/1000000 vectors...   Processed 22000/1000000 vectors...   Processed 32000/1000000 vectors...   Processed 42000/1000000 vectors...   Processed 52000/1000000 vectors...   Processed 62000/1000000 vectors...   Processed 72000/1000000 vectors...   Processed 82000/1000000 vectors...   Processed 92000/1000000 vectors...   Processed 102000/1000000 vectors...   Processed 112000/1000000 vectors...   Processed 122000/1000000 vectors...   Processed 132000/1000000 vectors...   Processed 142000/1000000 vectors...   Processed 152000/1000000 vectors...   Processed 162000/1000000 vectors...   Processed 172000/1000000 vectors...   Processed 182000/1000000 vectors...   Processed 192000/1000000 vectors...   Processed 202000/1000000 vectors...   Processed 212000/1000000 vectors...   Processed 222000/1000000 vectors...   Processed 232000/1000000 vectors...   Processed 242000/1000000 vectors...   Processed 252000/1000000 vectors...   Processed 262000/1000000 vectors...   Processed 272000/1000000 vectors...   Processed 282000/1000000 vectors...   Processed 292000/1000000 vectors...   Processed 302000/1000000 vectors...   Processed 312000/1000000 vectors...   Processed 322000/1000000 vectors...   Processed 332000/1000000 vectors...   Processed 342000/1000000 vectors...   Processed 352000/1000000 vectors...   Processed 362000/1000000 vectors...   Processed 372000/1000000 vectors...   Processed 382000/1000000 vectors...   Processed 392000/1000000 vectors...   Processed 402000/1000000 vectors...   Processed 412000/1000000 vectors...   Processed 422000/1000000 vectors...   Processed 432000/1000000 vectors...   Processed 442000/1000000 vectors...   Processed 452000/1000000 vectors...   Processed 462000/1000000 vectors...   Processed 472000/1000000 vectors...   Processed 482000/1000000 vectors...   Processed 492000/1000000 vectors...   Processed 502000/1000000 vectors...   Processed 512000/1000000 vectors...   Processed 522000/1000000 vectors...   Processed 532000/1000000 vectors...   Processed 542000/1000000 vectors...   Processed 552000/1000000 vectors...   Processed 562000/1000000 vectors...   Processed 572000/1000000 vectors...   Processed 582000/1000000 vectors...   Processed 592000/1000000 vectors...   Processed 602000/1000000 vectors...   Processed 612000/1000000 vectors...   Processed 622000/1000000 vectors...   Processed 632000/1000000 vectors...   Processed 642000/1000000 vectors...   Processed 652000/1000000 vectors...   Processed 662000/1000000 vectors...   Processed 672000/1000000 vectors...   Processed 682000/1000000 vectors...   Processed 692000/1000000 vectors...   Processed 702000/1000000 vectors...   Processed 712000/1000000 vectors...   Processed 722000/1000000 vectors...   Processed 732000/1000000 vectors...   Processed 742000/1000000 vectors...   Processed 752000/1000000 vectors...   Processed 762000/1000000 vectors...   Processed 772000/1000000 vectors...   Processed 782000/1000000 vectors...   Processed 792000/1000000 vectors...   Processed 802000/1000000 vectors...   Processed 812000/1000000 vectors...   Processed 822000/1000000 vectors...   Processed 832000/1000000 vectors...   Processed 842000/1000000 vectors...   Processed 852000/1000000 vectors...   Processed 862000/1000000 vectors...   Processed 872000/1000000 vectors...   Processed 882000/1000000 vectors...   Processed 892000/1000000 vectors...   Processed 902000/1000000 vectors...   Processed 912000/1000000 vectors...   Processed 922000/1000000 vectors...   Processed 932000/1000000 vectors...   Processed 942000/1000000 vectors...   Processed 952000/1000000 vectors...   Processed 962000/1000000 vectors...   Processed 972000/1000000 vectors...   Processed 982000/1000000 vectors...   Processed 992000/1000000 vectors...
âœ… Chroma: Insertion complete.
    Run 1: Recall@10=0.0998, Latency_p50=6.55ms
    Run 2: Recall@10=0.0998, Latency_p50=6.28ms
    Run 3: Recall@10=0.0998, Latency_p50=6.68ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: chroma_random    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.0998 â”‚
â”‚ Precision@10     â”‚ 0.9979 â”‚
â”‚ Recall@100       â”‚ 0.1968 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 6.68   â”‚
â”‚ Latency p99 (ms) â”‚ 8.92   â”‚
â”‚ QPS              â”‚ 149.0  â”‚
â”‚ Build Time (s)   â”‚ 743.35 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Recall is normalized by total
ground truth size (typically 
100). Precision is # Relevant
            / K.             
Exported JSON to results/chroma/random_chroma_results.json
Exported LaTeX tables to results/tables

Benchmark complete!
time="2026-01-17T10:11:46Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container chroma-server  Stopping
 Container chroma-server  Stopped
 Container chroma-server  Removing
 Container chroma-server  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-17T10:11:52Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container chroma-server  Creating
 Container chroma-server  Created
 Container chroma-server  Starting
 Container chroma-server  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: chroma
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: chroma on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Chroma: Inserting 100000 vectors in batches...
   Processed 2000/100000 vectors...   Processed 12000/100000 vectors...   Processed 22000/100000 vectors...   Processed 32000/100000 vectors...   Processed 42000/100000 vectors...   Processed 52000/100000 vectors...   Processed 62000/100000 vectors...   Processed 72000/100000 vectors...   Processed 82000/100000 vectors...   Processed 92000/100000 vectors...
âœ… Chroma: Insertion complete.
    Run 1: Recall@10=0.1000, Latency_p50=7.37ms
    Run 2: Recall@10=0.1000, Latency_p50=7.33ms
    Run 3: Recall@10=0.1000, Latency_p50=7.61ms
   Results: chroma_msmarco   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Precision@10     â”‚ 1.0000 â”‚
â”‚ Recall@100       â”‚ 0.9002 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 7.61   â”‚
â”‚ Latency p99 (ms) â”‚ 9.87   â”‚
â”‚ QPS              â”‚ 130.9  â”‚
â”‚ Build Time (s)   â”‚ 70.06  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Recall is normalized by total
ground truth size (typically 
100). Precision is # Relevant
            / K.             
Exported JSON to results/chroma/msmarco_chroma_results.json
Exported LaTeX tables to results/tables

Benchmark complete!
time="2026-01-17T10:14:24Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/chroma-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container chroma-server  Stopping
 Container chroma-server  Stopped
 Container chroma-server  Removing
 Container chroma-server  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-17T10:14:24Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container ragdbeval-weaviate-1  Creating
 Container ragdbeval-weaviate-1  Created
 Container ragdbeval-weaviate-1  Starting
 Container ragdbeval-weaviate-1  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: weaviate
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: weaviate on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Weaviate: Inserting 1000000 vectors (Fixed Batch Mode)...
   Processed 10000 vectors...   Processed 20000 vectors...   Processed 30000 vectors...   Processed 40000 vectors...   Processed 50000 vectors...   Processed 60000 vectors...   Processed 70000 vectors...   Processed 80000 vectors...   Processed 90000 vectors...   Processed 100000 vectors...   Processed 110000 vectors...   Processed 120000 vectors...   Processed 130000 vectors...   Processed 140000 vectors...   Processed 150000 vectors...   Processed 160000 vectors...   Processed 170000 vectors...   Processed 180000 vectors...   Processed 190000 vectors...   Processed 200000 vectors...   Processed 210000 vectors...   Processed 220000 vectors...   Processed 230000 vectors...   Processed 240000 vectors...   Processed 250000 vectors...   Processed 260000 vectors...   Processed 270000 vectors...   Processed 280000 vectors...   Processed 290000 vectors...   Processed 300000 vectors...   Processed 310000 vectors...   Processed 320000 vectors...   Processed 330000 vectors...   Processed 340000 vectors...   Processed 350000 vectors...   Processed 360000 vectors...   Processed 370000 vectors...   Processed 380000 vectors...   Processed 390000 vectors...   Processed 400000 vectors...   Processed 410000 vectors...   Processed 420000 vectors...   Processed 430000 vectors...   Processed 440000 vectors...   Processed 450000 vectors...   Processed 460000 vectors...   Processed 470000 vectors...   Processed 480000 vectors...   Processed 490000 vectors...   Processed 500000 vectors...   Processed 510000 vectors...   Processed 520000 vectors...   Processed 530000 vectors...   Processed 540000 vectors...   Processed 550000 vectors...   Processed 560000 vectors...   Processed 570000 vectors...   Processed 580000 vectors...   Processed 590000 vectors...   Processed 600000 vectors...   Processed 610000 vectors...   Processed 620000 vectors...   Processed 630000 vectors...   Processed 640000 vectors...   Processed 650000 vectors...   Processed 660000 vectors...   Processed 670000 vectors...   Processed 680000 vectors...   Processed 690000 vectors...   Processed 700000 vectors...   Processed 710000 vectors...   Processed 720000 vectors...   Processed 730000 vectors...   Processed 740000 vectors...   Processed 750000 vectors...   Processed 760000 vectors...   Processed 770000 vectors...   Processed 780000 vectors...   Processed 790000 vectors...   Processed 800000 vectors...   Processed 810000 vectors...   Processed 820000 vectors...   Processed 830000 vectors...   Processed 840000 vectors...   Processed 850000 vectors...   Processed 860000 vectors...   Processed 870000 vectors...   Processed 880000 vectors...   Processed 890000 vectors...   Processed 900000 vectors...   Processed 910000 vectors...   Processed 920000 vectors...   Processed 930000 vectors...   Processed 940000 vectors...   Processed 950000 vectors...   Processed 960000 vectors...   Processed 970000 vectors...   Processed 980000 vectors...   Processed 990000 vectors...
â³ Weaviate: Waiting for indexing (Shards READY)...
âœ… All shards READY.
    Run 1: Recall@10=0.1000, Latency_p50=14.06ms
    Run 2: Recall@10=0.1000, Latency_p50=14.85ms
    Run 3: Recall@10=0.1000, Latency_p50=14.48ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
  Results: weaviate_random   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Precision@10     â”‚ 1.0000 â”‚
â”‚ Recall@100       â”‚ 0.4525 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 14.48  â”‚
â”‚ Latency p99 (ms) â”‚ 19.44  â”‚
â”‚ QPS              â”‚ 70.3   â”‚
â”‚ Build Time (s)   â”‚ 792.66 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Recall is normalized by total
ground truth size (typically 
100). Precision is # Relevant
            / K.             
Exported JSON to results/weaviate/random_weaviate_results.json
Exported LaTeX tables to results/tables

Benchmark complete!
time="2026-01-17T10:35:55Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container ragdbeval-weaviate-1  Stopping
 Container ragdbeval-weaviate-1  Stopped
 Container ragdbeval-weaviate-1  Removing
 Container ragdbeval-weaviate-1  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-17T10:36:02Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container ragdbeval-weaviate-1  Creating
 Container ragdbeval-weaviate-1  Created
 Container ragdbeval-weaviate-1  Starting
 Container ragdbeval-weaviate-1  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: weaviate
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: weaviate on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Weaviate: Inserting 100000 vectors (Fixed Batch Mode)...
   Processed 10000 vectors...   Processed 20000 vectors...   Processed 30000 vectors...   Processed 40000 vectors...   Processed 50000 vectors...   Processed 60000 vectors...   Processed 70000 vectors...   Processed 80000 vectors...   Processed 90000 vectors...
â³ Weaviate: Waiting for indexing (Shards READY)...
âœ… All shards READY.
    Run 1: Recall@10=0.1000, Latency_p50=21.77ms
    Run 2: Recall@10=0.1000, Latency_p50=12.77ms
    Run 3: Recall@10=0.1000, Latency_p50=12.89ms
  Results: weaviate_msmarco  
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Precision@10     â”‚ 1.0000 â”‚
â”‚ Recall@100       â”‚ 0.9862 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 12.89  â”‚
â”‚ Latency p99 (ms) â”‚ 16.89  â”‚
â”‚ QPS              â”‚ 76.5   â”‚
â”‚ Build Time (s)   â”‚ 113.92 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Recall is normalized by total
ground truth size (typically 
100). Precision is # Relevant
            / K.             
Exported JSON to results/weaviate/msmarco_weaviate_results.json
Exported LaTeX tables to results/tables

Benchmark complete!
time="2026-01-17T10:39:42Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/weaviate-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container ragdbeval-weaviate-1  Stopping
 Container ragdbeval-weaviate-1  Stopped
 Container ragdbeval-weaviate-1  Removing
 Container ragdbeval-weaviate-1  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-17T10:39:44Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container qdrant_benchmark  Creating
 Container qdrant_benchmark  Created
 Container qdrant_benchmark  Starting
 Container qdrant_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: qdrant
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: qdrant on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Using Optimized Parallel Upload for 1000000 vectors...
Benchmark failed for config HNSW_L2: 'CollectionInfo' object has no attribute 'vectors_config'
Traceback (most recent call last):
  File "/home/ashenrashmike2000/RAGdbEval/src/benchmark/runner.py", line 241, in _run_single_benchmark
    stats = db.get_index_stats()
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/ashenrashmike2000/RAGdbEval/src/databases/qdrant_adapter.py", line 378, in get_index_stats
    if hasattr(info.vectors_config, 'params_map'): # For client < 1.7
               ^^^^^^^^^^^^^^^^^^^
  File "/home/ashenrashmike2000/.local/lib/python3.12/site-packages/pydantic/main.py", line 1026, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
AttributeError: 'CollectionInfo' object has no attribute 'vectors_config'
â³ Waiting for Qdrant indexing...
   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...
âœ… Optimization complete. Collection is GREEN.
Error with config HNSW_L2: 'CollectionInfo' object has no attribute 'vectors_config'
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
Exported JSON to results/qdrant/random_qdrant_results.json
Exported LaTeX tables to results/tables

Benchmark complete!
time="2026-01-17T10:45:01Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container qdrant_benchmark  Stopping
 Container qdrant_benchmark  Stopped
 Container qdrant_benchmark  Removing
 Container qdrant_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-17T10:45:06Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container qdrant_benchmark  Creating
 Container qdrant_benchmark  Created
 Container qdrant_benchmark  Starting
 Container qdrant_benchmark  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: qdrant
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: qdrant on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
âš¡ Auto-enabling Scalar Quantization (Int8) for speed...
ğŸš€ Using Optimized Parallel Upload for 100000 vectors...
Benchmark failed for config HNSW_Cosine: 'CollectionInfo' object has no attribute 'vectors_config'
Traceback (most recent call last):
  File "/home/ashenrashmike2000/RAGdbEval/src/benchmark/runner.py", line 241, in _run_single_benchmark
    stats = db.get_index_stats()
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/ashenrashmike2000/RAGdbEval/src/databases/qdrant_adapter.py", line 378, in get_index_stats
    if hasattr(info.vectors_config, 'params_map'): # For client < 1.7
               ^^^^^^^^^^^^^^^^^^^
  File "/home/ashenrashmike2000/.local/lib/python3.12/site-packages/pydantic/main.py", line 1026, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
AttributeError: 'CollectionInfo' object has no attribute 'vectors_config'
â³ Waiting for Qdrant indexing...
   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...   Status: yellow (Optimizing)...
âœ… Optimization complete. Collection is GREEN.
Error with config HNSW_Cosine: 'CollectionInfo' object has no attribute 'vectors_config'
Exported JSON to results/qdrant/msmarco_qdrant_results.json
Exported LaTeX tables to results/tables

Benchmark complete!
time="2026-01-17T10:46:20Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/qdrant-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container qdrant_benchmark  Stopping
 Container qdrant_benchmark  Stopped
 Container qdrant_benchmark  Removing
 Container qdrant_benchmark  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-17T10:46:20Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container milvus-minio  Creating
 Container milvus-etcd  Creating
 Container milvus-etcd  Created
 Container milvus-minio  Created
 Container milvus-standalone  Creating
 Container milvus-standalone  Created
 Container attu  Creating
 Container attu  Created
 Container milvus-etcd  Starting
 Container milvus-minio  Starting
 Container milvus-etcd  Started
 Container milvus-minio  Started
 Container milvus-standalone  Starting
 Container milvus-standalone  Started
 Container attu  Starting
 Container attu  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: milvus
Datasets: random
Runs per config: 3


Loading dataset: random

Benchmarking: milvus on random
  Vectors: (1000000, 128), Queries: (10000, 128)
  Metric: l2

  Index: HNSW_L2
    Building index...
ğŸš€ Milvus: Inserting 1000000 vectors in batches...
   Inserted batch 0 to 10000   Inserted batch 10000 to 20000   Inserted batch 20000 to 30000   Inserted batch 30000 to 40000   Inserted batch 40000 to 50000   Inserted batch 50000 to 60000   Inserted batch 60000 to 70000   Inserted batch 70000 to 80000   Inserted batch 80000 to 90000   Inserted batch 90000 to 100000   Inserted batch 100000 to 110000   Inserted batch 110000 to 120000   Inserted batch 120000 to 130000   Inserted batch 130000 to 140000   Inserted batch 140000 to 150000   Inserted batch 150000 to 160000   Inserted batch 160000 to 170000   Inserted batch 170000 to 180000   Inserted batch 180000 to 190000   Inserted batch 190000 to 200000   Inserted batch 200000 to 210000   Inserted batch 210000 to 220000   Inserted batch 220000 to 230000   Inserted batch 230000 to 240000   Inserted batch 240000 to 250000   Inserted batch 250000 to 260000   Inserted batch 260000 to 270000   Inserted batch 270000 to 280000   Inserted batch 280000 to 290000   Inserted batch 290000 to 300000   Inserted batch 300000 to 310000   Inserted batch 310000 to 320000   Inserted batch 320000 to 330000   Inserted batch 330000 to 340000   Inserted batch 340000 to 350000   Inserted batch 350000 to 360000   Inserted batch 360000 to 370000   Inserted batch 370000 to 380000   Inserted batch 380000 to 390000   Inserted batch 390000 to 400000   Inserted batch 400000 to 410000   Inserted batch 410000 to 420000   Inserted batch 420000 to 430000   Inserted batch 430000 to 440000   Inserted batch 440000 to 450000   Inserted batch 450000 to 460000   Inserted batch 460000 to 470000   Inserted batch 470000 to 480000   Inserted batch 480000 to 490000   Inserted batch 490000 to 500000   Inserted batch 500000 to 510000   Inserted batch 510000 to 520000   Inserted batch 520000 to 530000   Inserted batch 530000 to 540000   Inserted batch 540000 to 550000   Inserted batch 550000 to 560000   Inserted batch 560000 to 570000   Inserted batch 570000 to 580000   Inserted batch 580000 to 590000   Inserted batch 590000 to 600000   Inserted batch 600000 to 610000   Inserted batch 610000 to 620000   Inserted batch 620000 to 630000   Inserted batch 630000 to 640000   Inserted batch 640000 to 650000   Inserted batch 650000 to 660000   Inserted batch 660000 to 670000   Inserted batch 670000 to 680000   Inserted batch 680000 to 690000   Inserted batch 690000 to 700000   Inserted batch 700000 to 710000   Inserted batch 710000 to 720000   Inserted batch 720000 to 730000   Inserted batch 730000 to 740000   Inserted batch 740000 to 750000   Inserted batch 750000 to 760000   Inserted batch 760000 to 770000   Inserted batch 770000 to 780000   Inserted batch 780000 to 790000   Inserted batch 790000 to 800000   Inserted batch 800000 to 810000   Inserted batch 810000 to 820000   Inserted batch 820000 to 830000   Inserted batch 830000 to 840000   Inserted batch 840000 to 850000   Inserted batch 850000 to 860000   Inserted batch 860000 to 870000   Inserted batch 870000 to 880000   Inserted batch 880000 to 890000   Inserted batch 890000 to 900000   Inserted batch 900000 to 910000   Inserted batch 910000 to 920000   Inserted batch 920000 to 930000   Inserted batch 930000 to 940000   Inserted batch 940000 to 950000   Inserted batch 950000 to 960000   Inserted batch 960000 to 970000   Inserted batch 970000 to 980000   Inserted batch 980000 to 990000   Inserted batch 990000 to 1000000
âœ… Insertion complete.
ğŸ”¨ Milvus: Building index (HNSW)...
    Run 1: Recall@10=0.1000, Latency_p50=12.18ms
    Run 2: Recall@10=0.1000, Latency_p50=12.77ms
    Run 3: Recall@10=0.1000, Latency_p50=8.11ms
  Skipping HNSW_Cosine: Dataset requires 'l2', Config is 'cosine'
   Results: milvus_random    
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Precision@10     â”‚ 1.0000 â”‚
â”‚ Recall@100       â”‚ 0.3375 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 8.11   â”‚
â”‚ Latency p99 (ms) â”‚ 17.07  â”‚
â”‚ QPS              â”‚ 114.1  â”‚
â”‚ Build Time (s)   â”‚ 346.86 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Recall is normalized by total
ground truth size (typically 
100). Precision is # Relevant
            / K.             
Exported JSON to results/milvus/random_milvus_results.json
Exported LaTeX tables to results/tables

Benchmark complete!
time="2026-01-17T10:58:59Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container attu  Stopping
 Container attu  Stopped
 Container attu  Removing
 Container attu  Removed
 Container milvus-standalone  Stopping
 Container milvus-standalone  Stopped
 Container milvus-standalone  Removing
 Container milvus-standalone  Removed
 Container milvus-etcd  Stopping
 Container milvus-minio  Stopping
 Container milvus-etcd  Stopped
 Container milvus-etcd  Removing
 Container milvus-etcd  Removed
 Container milvus-minio  Stopped
 Container milvus-minio  Removing
 Container milvus-minio  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
time="2026-01-17T10:59:16Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Network ragdbeval_default  Creating
 Network ragdbeval_default  Created
 Container milvus-minio  Creating
 Container milvus-etcd  Creating
 Container milvus-etcd  Created
 Container milvus-minio  Created
 Container milvus-standalone  Creating
 Container milvus-standalone  Created
 Container attu  Creating
 Container attu  Created
 Container milvus-etcd  Starting
 Container milvus-minio  Starting
 Container milvus-minio  Started
 Container milvus-etcd  Started
 Container milvus-standalone  Starting
 Container milvus-standalone  Started
 Container attu  Starting
 Container attu  Started
============================================================
VectorDB Benchmark Framework
============================================================

VectorDB Benchmark
Databases: milvus
Datasets: msmarco
Runs per config: 3


Loading dataset: msmarco

Benchmarking: milvus on msmarco
Loading pre-computed embeddings (subset: 100000)...
  Vectors: (100000, 768), Queries: (1000, 768)
  Metric: cosine
  Skipping HNSW_L2: Dataset requires 'cosine', Config is 'l2'

  Index: HNSW_Cosine
    Building index...
ğŸš€ Milvus: Inserting 100000 vectors in batches...
   Inserted batch 0 to 10000   Inserted batch 10000 to 20000   Inserted batch 20000 to 30000   Inserted batch 30000 to 40000   Inserted batch 40000 to 50000   Inserted batch 50000 to 60000   Inserted batch 60000 to 70000   Inserted batch 70000 to 80000   Inserted batch 80000 to 90000   Inserted batch 90000 to 100000
âœ… Insertion complete.
ğŸ”¨ Milvus: Building index (HNSW)...
    Run 1: Recall@10=0.1000, Latency_p50=7.65ms
    Run 2: Recall@10=0.1000, Latency_p50=8.12ms
    Run 3: Recall@10=0.1000, Latency_p50=13.68ms
   Results: milvus_msmarco   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Recall@10        â”‚ 0.1000 â”‚
â”‚ Precision@10     â”‚ 1.0000 â”‚
â”‚ Recall@100       â”‚ 0.9620 â”‚
â”‚ MRR              â”‚ 1.0000 â”‚
â”‚ Latency p50 (ms) â”‚ 13.68  â”‚
â”‚ Latency p99 (ms) â”‚ 22.24  â”‚
â”‚ QPS              â”‚ 71.7   â”‚
â”‚ Build Time (s)   â”‚ 84.71  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Recall is normalized by total
ground truth size (typically 
100). Precision is # Relevant
            / K.             
Exported JSON to results/milvus/msmarco_milvus_results.json
Exported LaTeX tables to results/tables

Benchmark complete!
time="2026-01-17T11:01:48Z" level=warning msg="/home/ashenrashmike2000/RAGdbEval/milvus-docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
 Container attu  Stopping
 Container attu  Stopped
 Container attu  Removing
 Container attu  Removed
 Container milvus-standalone  Stopping
 Container milvus-standalone  Stopped
 Container milvus-standalone  Removing
 Container milvus-standalone  Removed
 Container milvus-minio  Stopping
 Container milvus-etcd  Stopping
 Container milvus-etcd  Stopped
 Container milvus-etcd  Removing
 Container milvus-etcd  Removed
 Container milvus-minio  Stopped
 Container milvus-minio  Removing
 Container milvus-minio  Removed
 Network ragdbeval_default  Removing
 Network ragdbeval_default  Removed
ğŸ“Š Generating plots from /home/ashenrashmike2000/RAGdbEval/results/results.csv...
âœ… Successfully generated 16 plots in /home/ashenrashmike2000/RAGdbEval/results/plots
ğŸ§¹ Cleaning up workspace...
ğŸ§¹ Deleting old results.csv file...

============================================================
 PREPARING BENCHMARK FOR: CHROMA
============================================================
ğŸš€ Starting chroma...
â³ Waiting 30s for chroma...

â–¶ï¸  Running: chroma on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database chroma --dataset random --runs 3 --export json latex
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting chroma...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for chroma...

â–¶ï¸  Running: chroma on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database chroma --dataset msmarco --runs 3 --export json latex
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping chroma...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: WEAVIATE
============================================================
ğŸš€ Starting weaviate...
â³ Waiting 30s for weaviate...

â–¶ï¸  Running: weaviate on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database weaviate --dataset random --runs 3 --export json latex
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting weaviate...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for weaviate...

â–¶ï¸  Running: weaviate on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database weaviate --dataset msmarco --runs 3 --export json latex
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping weaviate...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: QDRANT
============================================================
ğŸš€ Starting qdrant...
â³ Waiting 30s for qdrant...

â–¶ï¸  Running: qdrant on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database qdrant --dataset random --runs 3 --export json latex
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting qdrant...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Waiting 30s for qdrant...

â–¶ï¸  Running: qdrant on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database qdrant --dataset msmarco --runs 3 --export json latex
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping qdrant...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
 PREPARING BENCHMARK FOR: MILVUS
============================================================
ğŸš€ Starting milvus...
â³ Polling Milvus port 19530...
âœ… Milvus is ready!

â–¶ï¸  Running: milvus on random...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database milvus --dataset random --runs 3 --export json latex
âœ… Benchmark Run Successful.
â™»ï¸  RESET: Restarting milvus...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...
â³ Polling Milvus port 19530...
âœ… Milvus is ready!

â–¶ï¸  Running: milvus on msmarco...
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/run_benchmark.py" --database milvus --dataset msmarco --runs 3 --export json latex
âœ… Benchmark Run Successful.
ğŸ›‘ Stopping milvus...
ğŸ§¹ Deleting local volumes folder: /home/ashenrashmike2000/RAGdbEval/volumes...

============================================================
ğŸ“„ COMPILING CSV
============================================================
ğŸ” Finding all benchmark JSON files to compile...
Found 42 JSON files to compile.

============================================================
ğŸ“Š GENERATING PLOTS
============================================================
Executing: "/usr/bin/python" "/home/ashenrashmike2000/RAGdbEval/scripts/generate_plots.py" --database chroma weaviate qdrant milvus --dataset random msmarco
